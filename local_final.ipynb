{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T17:44:47.816485Z",
     "iopub.status.busy": "2024-04-12T17:44:47.816053Z",
     "iopub.status.idle": "2024-04-12T17:44:48.427534Z",
     "shell.execute_reply": "2024-04-12T17:44:48.425757Z",
     "shell.execute_reply.started": "2024-04-12T17:44:47.816450Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Ram Ram\n",
    "# import pickle\n",
    "# with open(\"/kaggle/input/duc2002dataandwheels/KaggleWheels/datasets/extracts_duc_filenames.pickle\",\"rb\") as file:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T14:37:03.113559Z",
     "iopub.status.busy": "2024-03-17T14:37:03.112460Z",
     "iopub.status.idle": "2024-03-17T14:37:07.080261Z",
     "shell.execute_reply": "2024-03-17T14:37:07.075722Z",
     "shell.execute_reply.started": "2024-03-17T14:37:03.113516Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shari\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ram Ram\n",
    "if(False):\n",
    "    !pip install keybert\n",
    "    !pip install sentence_transformers\n",
    "    !pip install rouge_score\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        os.makedirs(\"/kaggle/working/results\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(\"/kaggle/working/details\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    import os\n",
    "    pack_list=[]\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.whl'):\n",
    "                pack_list.append(os.path.join(dirname,filename))\n",
    "    \n",
    "    for item in pack_list:\n",
    "        !pip install item\n",
    "    \n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import re\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import random\n",
    "from itertools import combinations\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from rouge_score import rouge_scorer\n",
    "from datetime import datetime\n",
    "from keybert import KeyBERT\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "import random\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import networkx as nx\n",
    "from csv import writer\n",
    "\n",
    "\n",
    "import random\n",
    "from itertools import combinations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.081641Z",
     "iopub.status.idle": "2024-03-17T14:37:07.082476Z",
     "shell.execute_reply": "2024-03-17T14:37:07.082249Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.082230Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install lexrank\n",
    "# !pip install tomotopy\n",
    "from lexrank import LexRank\n",
    "from lexrank.mappings.stopwords import STOPWORDS\n",
    "from path import Path\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import tomotopy as tp\n",
    "from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.084063Z",
     "iopub.status.idle": "2024-03-17T14:37:07.084399Z",
     "shell.execute_reply": "2024-03-17T14:37:07.084250Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.084237Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_specials(text):\n",
    "    text=text.replace(',', ' ')\n",
    "    text=text.replace(':', ' ')\n",
    "    text=text.replace(';', ' ')\n",
    "    text=text.replace('\\n', ' ')\n",
    "    text=text.replace('\\t', ' ')\n",
    "    return text\n",
    "\n",
    "def remove_special_chars(input_string):\n",
    "    # Exclude ? ! ' . from the pattern\n",
    "    pattern = r'[^\\w\\s?!\\'.\"]'\n",
    "    result = re.sub(pattern, ' ', input_string)\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        file_content = file.read()\n",
    "    return file_content\n",
    "\n",
    "def segmentation(input_stream):\n",
    "    # Split based on . ? ! '\n",
    "    delimiters = ['.', '?', '!']\n",
    "    # Create a pattern for the regex to split on any of the delimiters\n",
    "    pattern = '|'.join(map(re.escape, delimiters))\n",
    "    list_of_sentences = re.split(pattern, str(input_stream))\n",
    "\n",
    "    # Remove empty and whitespace-only strings from the list\n",
    "    list_of_sentences = [sentence.strip() for sentence in list_of_sentences if sentence.strip() != \"\"]\n",
    "\n",
    "    return list_of_sentences\n",
    "\n",
    "def tokenization(sentence_list,token_check=True):\n",
    "    sentence_word_list=[]\n",
    "    for sentence in sentence_list:\n",
    "        sentence_word_list.append(word_tokenize(sentence))\n",
    "        if token_check==True:\n",
    "            new_word_list=[]\n",
    "            for i in range(len(sentence_word_list[-1])):\n",
    "                if sentence_word_list[-1][i].isalnum():\n",
    "                    new_word_list.append(sentence_word_list[-1][i])\n",
    "            sentence_word_list[-1]=new_word_list\n",
    "        for i in range(len(sentence_word_list[-1])):\n",
    "            sentence_word_list[-1][i]=sentence_word_list[-1][i].strip().lower()\n",
    "    \n",
    "    return sentence_word_list\n",
    "\n",
    "def remove_stop_words(word_list):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_word_list=[]\n",
    "    for word in word_list:\n",
    "        if word not in stop_words:\n",
    "            new_word_list.append(word)\n",
    "    return new_word_list\n",
    "\n",
    "def root_lemmatizer(word_list):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "    porter = PorterStemmer()\n",
    "    stemmed_words = [porter.stem(word) for word in word_list]\n",
    "#     lemmatized_words = [lemmatizer.lemmatize(word,pos='n') for word in word_list]\n",
    "    #stemmed_words = [porter.stem(word) for word in lemmatized_words]\n",
    "    return stemmed_words # lemmatized_words  \n",
    "\n",
    "\n",
    "def remove_stop_words_spacy(word_list):\n",
    "    stop_words = set(nlp.Defaults.stop_words)\n",
    "    new_word_list = [word for word in word_list if word not in stop_words]\n",
    "    return new_word_list\n",
    "\n",
    "def porter_stemmer_spacy(word_list):\n",
    "    # Since SpaCy uses lemmatization instead of stemming, \n",
    "    # we can use the SpaCy lemmatizer to get the base form of words.\n",
    "    stemmed_words = [token.lemma_ for token in nlp(\" \".join(word_list))]\n",
    "    return stemmed_words\n",
    "\n",
    "def tokenization_spacy(sentence_list,token_check=True):\n",
    "    sentence_word_list = []\n",
    "    for sentence in sentence_list:\n",
    "        doc = nlp(sentence)\n",
    "        new_word_list = [token.text.lower() for token in doc if token.is_alpha]\n",
    "        if token_check==True:\n",
    "            new_word_list_spacy=[]\n",
    "            for i in range(len(new_word_list)):\n",
    "                if new_word_list[i].isalnum():\n",
    "                    new_word_list_spacy.append(new_word_list[i])\n",
    "            sentence_word_list.append(new_word_list_spacy)\n",
    "        else:\n",
    "            sentence_word_list.append(new_word_list)\n",
    "        \n",
    "        for i in range(len(sentence_word_list[-1])):\n",
    "            sentence_word_list[-1][i]=sentence_word_list[-1][i].strip().lower()\n",
    "    return sentence_word_list\n",
    "\n",
    "\n",
    "def preprocess(input_stream,spacy_use=False,token_check=True,stop_check=True,stem_check=True):\n",
    "    processed_input_stream=segmentation(input_stream)\n",
    "    NPIS=[]\n",
    "    \n",
    "    for i in range(len(processed_input_stream)):\n",
    "        if len(processed_input_stream[i].split()) >=5:\n",
    "            NPIS.append(processed_input_stream[i])\n",
    "    processed_input_stream=NPIS.copy()\n",
    "    \n",
    "\n",
    "    if spacy_use==False:\n",
    "        word_lists=tokenization(processed_input_stream,token_check)\n",
    "        new_word_lists=[]\n",
    "        negation=[]\n",
    "        for i in range(len(word_lists)):\n",
    "            l=word_lists[i]\n",
    "            new_l=remove_stop_words(l)\n",
    "            if stop_check==True:\n",
    "                if len(new_l)>=1:\n",
    "                    new_word_lists.append(new_l)\n",
    "                else:\n",
    "                    negation.append(i)\n",
    "            else:\n",
    "                new_word_lists.append(new_l)\n",
    "        \n",
    "        new_processed_input_stream=[]\n",
    "        for i in range(len(processed_input_stream)):\n",
    "            if i not in negation:\n",
    "                new_processed_input_stream.append(processed_input_stream[i])\n",
    "        processed_input_stream=new_processed_input_stream.copy()\n",
    "        negation=[]\n",
    "        stemmed_word_lists=[]\n",
    "        for i in range(len(new_word_lists)):\n",
    "            l=new_word_lists[i]\n",
    "            new_l=root_lemmatizer(l)\n",
    "            if stem_check==True:\n",
    "                if len(new_l)>=1:\n",
    "                    stemmed_word_lists.append(new_l)\n",
    "                else:\n",
    "                    negation.append(i)\n",
    "            else:\n",
    "                stemmed_word_lists.append(new_l)\n",
    "                \n",
    "        new_processed_input_stream=[]\n",
    "        for i in range(len(processed_input_stream)):\n",
    "            if i not in negation:\n",
    "                new_processed_input_stream.append(processed_input_stream[i])\n",
    "        processed_input_stream=new_processed_input_stream.copy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        NPIS=[]\n",
    "        new_SWL=[]\n",
    "        for i in range(len(stemmed_word_lists)):\n",
    "            if len(stemmed_word_lists[i]) >=3:\n",
    "                NPIS.append(processed_input_stream[i])\n",
    "                new_SWL.append(stemmed_word_lists[i])\n",
    "            else:\n",
    "                pass\n",
    "        processed_input_stream=NPIS.copy()\n",
    "        stemmed_word_lists=new_SWL.copy()\n",
    "        return stemmed_word_lists,processed_input_stream\n",
    "    else:\n",
    "        word_lists=tokenization_spacy(processed_input_stream,token_check)\n",
    "        new_word_lists=[]\n",
    "        negation=[]\n",
    "        for i in range(len(word_lists)):\n",
    "            l=word_lists[i]\n",
    "            new_l=remove_stop_words_spacy(l)\n",
    "            if stop_check==True:\n",
    "                if len(new_l)>=1:\n",
    "                    new_word_lists.append(new_l)\n",
    "                else:\n",
    "                    negation.append(i)\n",
    "            else:\n",
    "                new_word_lists.append(new_l)\n",
    "\n",
    "        \n",
    "        new_processed_input_stream=[]\n",
    "        for i in range(len(processed_input_stream)):\n",
    "            if i not in negation:\n",
    "                new_processed_input_stream.append(processed_input_stream[i])\n",
    "        processed_input_stream=new_processed_input_stream.copy()\n",
    "        negation=[]\n",
    "        stemmed_word_lists=[]\n",
    "        for i in range(len(new_word_lists)):\n",
    "            l=new_word_lists[i]\n",
    "            new_l=porter_stemmer_spacy(l)\n",
    "            if stem_check==True:\n",
    "                if len(new_l)>=1:\n",
    "                    stemmed_word_lists.append(new_l)\n",
    "                else:\n",
    "                    negation.append(i)\n",
    "            else:\n",
    "                stemmed_word_lists.append(new_l)\n",
    "                \n",
    "        new_processed_input_stream=[]\n",
    "        for i in range(len(processed_input_stream)):\n",
    "            if i not in negation:\n",
    "                new_processed_input_stream.append(processed_input_stream[i])\n",
    "        processed_input_stream=new_processed_input_stream.copy()\n",
    "        \n",
    "        NPIS=[]\n",
    "        new_SWL=[]\n",
    "        for i in range(len(stemmed_word_lists)):\n",
    "            if len(stemmed_word_lists[i]) >=3:\n",
    "                NPIS.append(processed_input_stream[i])\n",
    "                new_SWL.append(stemmed_word_lists[i])\n",
    "            else:\n",
    "                pass\n",
    "        processed_input_stream=NPIS.copy()\n",
    "        stemmed_word_lists=new_SWL.copy()\n",
    "        \n",
    "        \n",
    "        return stemmed_word_lists,processed_input_stream\n",
    "\n",
    "# def zeroprocess(input_stream):\n",
    "#     processed_input_stream=segmentation(input_stream)\n",
    "#     revised_input_stream=\"\"\n",
    "#     for item in processed_input_stream:\n",
    "#         revised_input_stream+=remove_special_chars(item)+\" \\n \"\n",
    "#     revised_input_stream=revised_input_stream[:-1]\n",
    "#     return revised_input_stream\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.086069Z",
     "iopub.status.idle": "2024-03-17T14:37:07.086449Z",
     "shell.execute_reply": "2024-03-17T14:37:07.086282Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.086268Z"
    }
   },
   "outputs": [],
   "source": [
    "def tfik(word_sentence_list):\n",
    "    tfik_dict={}\n",
    "    for word in word_sentence_list:\n",
    "        \n",
    "        tfik_dict[word]=tfik_dict.get(word,0)+1\n",
    "#         print(tfik_dict,word,word_sentence_list)\n",
    "    return tfik_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.088113Z",
     "iopub.status.idle": "2024-03-17T14:37:07.088464Z",
     "shell.execute_reply": "2024-03-17T14:37:07.088306Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.088291Z"
    }
   },
   "outputs": [],
   "source": [
    "def distinct_terms(tfik_dict):\n",
    "    terms_dict={}\n",
    "    for k,v in tfik_dict.items():\n",
    "        for word,ctr in v.items():\n",
    "            terms_dict[word]=terms_dict.get(word,0)+1\n",
    "            \n",
    "    return terms_dict\n",
    "\n",
    "\n",
    "def indices_terms_mapper(terms_dict):\n",
    "    index_to_term_mapper={}\n",
    "    term_to_index_mapper={}\n",
    "    for i in range(len(list(terms_dict.keys()))):\n",
    "        index_to_term_mapper[i]=list(terms_dict.keys())[i]\n",
    "        term_to_index_mapper[list(terms_dict.keys())[i]]=i\n",
    "    return  index_to_term_mapper, term_to_index_mapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.089374Z",
     "iopub.status.idle": "2024-03-17T14:37:07.089952Z",
     "shell.execute_reply": "2024-03-17T14:37:07.089763Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.089746Z"
    }
   },
   "outputs": [],
   "source": [
    "def wik(tfik_dict,terms_dict,index_to_term_mapper,term_to_index_mapper,no_of_sentences,no_of_terms):\n",
    "\n",
    "    \n",
    "    si_vec_dict={}\n",
    "    for sentence_index in range(no_of_sentences):\n",
    "        si_vec_dict[sentence_index]={}\n",
    "        for term_index in range(no_of_terms):\n",
    "            si_vec_dict[sentence_index][term_index]=0.0\n",
    "    for sentence_index, occurrence_dict in tfik_dict.items():\n",
    "        for word,count in occurrence_dict.items():\n",
    "            si_vec_dict[sentence_index][term_to_index_mapper[word]]=count * math.log(no_of_sentences/terms_dict[word],10)\n",
    "\n",
    "    return si_vec_dict\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.091018Z",
     "iopub.status.idle": "2024-03-17T14:37:07.091832Z",
     "shell.execute_reply": "2024-03-17T14:37:07.091653Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.091633Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_o(wik_dict,no_of_sentences,no_of_terms):\n",
    "    o_dict={}\n",
    "\n",
    "    for term_index in range(no_of_terms):\n",
    "        o_dict[term_index]=0\n",
    "\n",
    "    for k,v in wik_dict.items():\n",
    "        for term_index, wik_value in v.items():\n",
    "\n",
    "            o_dict[term_index]+=wik_value\n",
    "    for term_index in range(no_of_terms):\n",
    "        o_dict[term_index]/=no_of_sentences\n",
    "\n",
    "    return o_dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.092896Z",
     "iopub.status.idle": "2024-03-17T14:37:07.093563Z",
     "shell.execute_reply": "2024-03-17T14:37:07.093396Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.093380Z"
    }
   },
   "outputs": [],
   "source": [
    "def cos_sim(vec_a,vec_b):\n",
    "    return dot(vec_a, vec_b)/(norm(vec_a)*norm(vec_b))\n",
    "\n",
    "def coverage(x,doc_info):\n",
    "    wik_dict=doc_info.wik_dict\n",
    "    o_dict=doc_info.o_dict\n",
    "    kw_topic_info=doc_info.kw_topic_info\n",
    "    coverage_value=0\n",
    "    for i in range(len(x)):\n",
    "        coverage_value += doc_info.o_wik_sim[i] * x[i]\n",
    "    return coverage_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.095444Z",
     "iopub.status.idle": "2024-03-17T14:37:07.095838Z",
     "shell.execute_reply": "2024-03-17T14:37:07.095666Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.095651Z"
    }
   },
   "outputs": [],
   "source": [
    "# def kw_multiplier(i,j,kw_topic_info):\n",
    "#     return kw_topic_info.kw_multipliers_dict[(i,j)]\n",
    "# def topic_multiplier(i,j,kw_topic_info):\n",
    "#     return kw_topic_info.topic_multipliers_dict[(i,j)]\n",
    "\n",
    "# def compare_vectors(vec1, vec2, val1=2, val2=1/2):\n",
    "#     for i in range(len(vec1)):\n",
    "#         if vec1[i] == 1 and vec2[i] == 1:\n",
    "#             return val1\n",
    "#     return val2\n",
    "\n",
    "# def relevance(x,doc_info):\n",
    "#     sum_vec={}\n",
    "#     for k1,v1 in doc_info.o_dict.items():\n",
    "#         sum_vec[k1]=0\n",
    "#     for k1,v1 in doc_info.wik_dict.items():\n",
    "#         for k2,v2 in v1.items():\n",
    "#             sum_vec[k2]+=(v2 * x[k1])\n",
    "#     for k1,v1 in doc_info.o_dict.items():\n",
    "#         sum_vec[k1]/=doc_info.no_of_sentences\n",
    "        \n",
    "#     term_1=cos_sim(list(sum_vec.values()),doc_info.o_dict.values())\n",
    "#     term_2=0\n",
    "#     for i in range(len(doc_info.no_of_sentences)):\n",
    "#         term_2+=(cos_sim(list(sum_vec.values()),list(wik_dict[i].values())) * x[i])\n",
    "    \n",
    "#     return term_1*term_2\n",
    "\n",
    "    \n",
    "def redundancy_reduction(x,doc_info):\n",
    "    wik_dict=doc_info.wik_dict\n",
    "    kw_topic_info=doc_info.kw_topic_info\n",
    "    \n",
    "#     time1=timeit.default_timer()\n",
    "    redundancy_reduction_value=0\n",
    "    for i in range(len(x)-1):\n",
    "        for j in range(i+1,len(x)):\n",
    "            redundancy_reduction_value += ((doc_info.wik_sim[(i,j)] *x[i]*x[j]))\n",
    "    \n",
    "    redundancy_reduction_value *= np.sum(x)\n",
    "    redundancy_reduction_value=1/(1+redundancy_reduction_value)\n",
    "#     time2=timeit.default_timer()\n",
    "#     print(\"Inside Redundancy Reduction :\",time2-time1)\n",
    "    return redundancy_reduction_value\n",
    "# print(redundancy_reduction(some_x_vec,wik_dict))\n",
    "   \n",
    "def redundancy_reduction_topic(x,doc_info):\n",
    "    wik_dict=doc_info.wik_dict\n",
    "    kw_topic_info=doc_info.kw_topic_info\n",
    "    \n",
    "#     time1=timeit.default_timer()\n",
    "    redundancy_reduction_value=0\n",
    "    for i in range(len(x)-1):\n",
    "        for j in range(i+1,len(x)):\n",
    "            redundancy_reduction_value += ((doc_info.t_wik_sim[(i,j)] *x[i]*x[j])) \n",
    "    \n",
    "    redundancy_reduction_value *= np.sum(x)\n",
    "    redundancy_reduction_value=1/(1+redundancy_reduction_value)\n",
    "#     time2=timeit.default_timer()\n",
    "#     print(\"Inside Redundancy Reduction :\",time2-time1)\n",
    "    return redundancy_reduction_value\n",
    "# print(redundancy_reduction(some_x_vec,wik_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.097100Z",
     "iopub.status.idle": "2024-03-17T14:37:07.097438Z",
     "shell.execute_reply": "2024-03-17T14:37:07.097279Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.097266Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_NDS_consensus(NDS_archive):\n",
    "    sentence_count_dict={}\n",
    "    for i in range(len(NDS_archive[0].mem)):\n",
    "        sentence_count_dict[i]=0\n",
    "    for item in NDS_archive:\n",
    "        for i in range(len(item.mem)):\n",
    "            if item.mem[i]==1:\n",
    "                sentence_count_dict[i]+=1\n",
    "    return sentence_count_dict\n",
    "        \n",
    "\n",
    "def mutation_operator_v1(x, doc_info):\n",
    "    revised_x = x.copy()\n",
    "    wik_dict=doc_info.wik_dict\n",
    "    o_dict=doc_info.o_dict\n",
    "    pm=doc_info.pm\n",
    "    \n",
    "    RHS = np.mean([doc_info.o_wik_sim[j] for j in range(len(x))])\n",
    "\n",
    "    mutation_probabilities = np.random.random(len(x))\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if mutation_probabilities[i] <= pm:\n",
    "\n",
    "            LHS = doc_info.o_wik_sim[i]\n",
    "            if LHS >= RHS:\n",
    "                revised_x[i] = 1\n",
    "            else:\n",
    "                revised_x[i] = 0\n",
    "    revised_x=fast_repair_operator(revised_x.copy(),doc_info)[0]\n",
    "    return revised_x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mutation_operator_v2(x, doc_info):\n",
    "    revised_x = x.copy()\n",
    "    wik_dict=doc_info.wik_dict\n",
    "    o_dict=doc_info.o_dict\n",
    "    pm=doc_info.pm\n",
    "    \n",
    "    RHS = np.mean([doc_info.o_wik_sim[j] for j in range(len(x))])\n",
    "\n",
    "    mutation_probabilities = np.random.random(len(x))\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if mutation_probabilities[i] <= pm:\n",
    "\n",
    "            LHS = doc_info.o_wik_sim[i]\n",
    "            if LHS >= RHS:\n",
    "                revised_x[i] = 1\n",
    "            else:\n",
    "                revised_x[i] = 0\n",
    "    \n",
    "    if(len(doc_info.NDS_archive)!=0):\n",
    "        sentence_consensus = get_NDS_consensus(doc_info.NDS_archive)\n",
    "        sentence_consensus_sorted = dict(sorted(sentence_consensus.items(), key=lambda item: item[1],reverse=True))\n",
    "        top=min(int(doc_info.no_of_sentences/10),doc_info.no_of_sentences)\n",
    "        top_sentences=list(sentence_consensus_sorted.keys())[:top]\n",
    "        for ind in top_sentences:\n",
    "            if random.random()<pm:\n",
    "                revised_x[ind]=1\n",
    "    revised_x=fast_repair_operator(revised_x.copy(),doc_info)[0]\n",
    "    return revised_x\n",
    "\n",
    "def mutation_operator_select(x,doc_info):\n",
    "    mutation_operator_version=doc_info.mutation_operator_version\n",
    "    \n",
    "    if mutation_operator_version==1:\n",
    "        return mutation_operator_v1(x,doc_info)\n",
    "    elif mutation_operator_version==2:\n",
    "        return mutation_operator_v2(x,doc_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.098568Z",
     "iopub.status.idle": "2024-03-17T14:37:07.098924Z",
     "shell.execute_reply": "2024-03-17T14:37:07.098779Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.098765Z"
    }
   },
   "outputs": [],
   "source": [
    "def term_indices_of_sentences(sentences,term_to_index_mapper):\n",
    "    term_sentence_dict={}\n",
    "    for i in range(len(sentences)):\n",
    "        term_sentence_dict[i]=[]\n",
    "        sentence=sentences[i]\n",
    "        for term in sentence:\n",
    "            term_sentence_dict[i].append(term_to_index_mapper[term])\n",
    "        term_sentence_dict[i]=sorted(term_sentence_dict[i])\n",
    "    return term_sentence_dict\n",
    "\n",
    "\n",
    "def ovec_summary_modifier(o_dict,x,wik_dict):\n",
    "    o_dict_temp={}\n",
    "    for term_index in wik_dict[0].keys():\n",
    "        o_dict_temp[term_index]=0\n",
    "        for sentence_index in wik_dict.keys():\n",
    "            o_dict_temp[term_index]+=(wik_dict[sentence_index][term_index]*x[sentence_index])\n",
    "    return o_dict_temp\n",
    "\n",
    "def term_combined_list_former(set_of_sentences_indices,term_index_sentence_dict):\n",
    "    combined=[]\n",
    "    for sentence_index in set_of_sentences_indices:\n",
    "        for term_index in term_index_sentence_dict[sentence_index]:\n",
    "            if term_index not in combined:\n",
    "                combined.append(term_index)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.100463Z",
     "iopub.status.idle": "2024-03-17T14:37:07.100811Z",
     "shell.execute_reply": "2024-03-17T14:37:07.100663Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.100649Z"
    }
   },
   "outputs": [],
   "source": [
    "def repair_operator(x,doc_info):  #term_index_sentence_dict is not there as of now\n",
    "    preprocessed_inputs=doc_info.preprocessed_inputs\n",
    "    list_of_sentences=doc_info.list_of_sentences\n",
    "    wik_dict=doc_info.wik_dict\n",
    "    o_dict=doc_info.o_dict\n",
    "    epsilon=doc_info.epsilon\n",
    "    delta=doc_info.delta\n",
    "    L=doc_info.L\n",
    "    \n",
    "    cur_length_of_summary=0\n",
    "    for i in range(len(x)):\n",
    "        if x[i]==1:\n",
    "            cur_length_of_summary+=doc_info.sentence_lengths[i]\n",
    "                      \n",
    "    lower_bound=L-epsilon\n",
    "    upper_bound=L+epsilon\n",
    "    \n",
    "    if cur_length_of_summary >=lower_bound and cur_length_of_summary<= upper_bound:\n",
    "        return (x,True)\n",
    "    \n",
    "    if cur_length_of_summary<lower_bound:\n",
    "        return (x,True)\n",
    "\n",
    "    \n",
    "    if cur_length_of_summary >=lower_bound and cur_length_of_summary> upper_bound:\n",
    "        \n",
    "        #Repair needed\n",
    "        trial = 0\n",
    "        max_trials=min((len(preprocessed_inputs)**2) ,300)\n",
    "        print(\"Repair Trials : \",max_trials)\n",
    "        while cur_length_of_summary> upper_bound and trial<=max_trials:\n",
    "            summary_indices = [index for index, value in enumerate(x) if value == 1]\n",
    "            try:\n",
    "                chosen_combination= random.choice(list(combinations(summary_indices,2)))\n",
    "            except:\n",
    "                return (x,True)\n",
    "            i=chosen_combination[0]\n",
    "            j=chosen_combination[1]\n",
    "            if i!=j and x[i]==1 and x[j]==1 and cos_sim(list(wik_dict[i].values()),list(wik_dict[j].values())) >= delta:\n",
    "\n",
    "                si=list(dict(sorted(wik_dict[i].items())).values())\n",
    "                sj=list(dict(sorted(wik_dict[j].items())).values())\n",
    "                o_vec=list(dict(sorted(o_dict.items())).values())\n",
    "\n",
    "                x_without_i=x.copy()\n",
    "                x_without_i[i]=0\n",
    "                x_without_j=x.copy()\n",
    "                x_without_j[j]=0\n",
    "\n",
    "                o_sum_vec=list(dict(sorted(ovec_summary_modifier(o_dict,x,wik_dict).items())).values())\n",
    "                o_sum_vec_without_i=list(dict(sorted(ovec_summary_modifier(o_dict,x_without_i,wik_dict).items())).values())\n",
    "                o_sum_vec_without_j=list(dict(sorted(ovec_summary_modifier(o_dict,x_without_j,wik_dict).items())).values())\n",
    "\n",
    "                score_si=cos_sim(si,o_vec) + (cos_sim(o_sum_vec,o_vec)-cos_sim(o_sum_vec_without_i,o_vec))*10\n",
    "                score_sj=cos_sim(sj,o_vec) + (cos_sim(o_sum_vec,o_vec)-cos_sim(o_sum_vec_without_j,o_vec))*10\n",
    "                \n",
    "                if score_si >= score_sj:\n",
    "                    #Discard sj, Keep si\n",
    "                    x[j]=0\n",
    "                    cur_length_of_summary-=doc_info.sentence_lengths[i]\n",
    "                else:\n",
    "                    x[i]=0\n",
    "                    cur_length_of_summary-=doc_info.sentence_lengths[i]\n",
    "        \n",
    "            trial+=1\n",
    "\n",
    "        return (x,True)\n",
    "\n",
    "\n",
    "def select_and_return_scores(doc_info):\n",
    "    scores={}\n",
    "    score_types=[\"textrank\",\"tf\",\"graph\",\"lexrank\"]\n",
    "    score_type_chosen=random.choice(score_types)\n",
    "    if score_type_chosen==\"tf\":\n",
    "        scores=doc_info.scores_details.tf_scores\n",
    "    elif score_type_chosen==\"textrank\":\n",
    "        scores=doc_info.scores_details.textrank_scores\n",
    "    elif score_type_chosen==\"graph\":\n",
    "        scores=doc_info.scores_details.graph_scores\n",
    "    elif score_type_chosen==\"lexrank\":\n",
    "        scores=doc_info.scores_details.lexrank_scores\n",
    "    return scores\n",
    "def fast_repair_operator(x,doc_info):\n",
    "    preprocessed_inputs=doc_info.preprocessed_inputs\n",
    "    list_of_sentences=doc_info.list_of_sentences\n",
    "    wik_dict=doc_info.wik_dict\n",
    "    o_dict=doc_info.o_dict\n",
    "    epsilon=doc_info.epsilon\n",
    "    delta=doc_info.delta\n",
    "    L=doc_info.L\n",
    "    \n",
    "    cur_length_of_summary=0\n",
    "    for i in range(len(x)):\n",
    "        if x[i]==1:\n",
    "            cur_length_of_summary+=doc_info.sentence_lengths[i]\n",
    "    \n",
    "    lower_bound = L - epsilon\n",
    "    upper_bound = L + epsilon\n",
    "\n",
    "    scores=select_and_return_scores(doc_info)\n",
    "        \n",
    "    present_scores={}\n",
    "    absent_scores={}\n",
    "    for i in range(len(x)):\n",
    "        if x[i]==1:\n",
    "            present_scores[i]=scores[i]\n",
    "        else:\n",
    "            absent_scores[i]=scores[i]\n",
    "    \n",
    "    if cur_length_of_summary >=lower_bound and cur_length_of_summary<= upper_bound:\n",
    "        return (x,True)\n",
    "    \n",
    "\n",
    "    elif cur_length_of_summary<lower_bound:\n",
    "        \n",
    "\n",
    "        while cur_length_of_summary < lower_bound:\n",
    "            if len(list(absent_scores.keys())) <=0:\n",
    "                return (x,True)\n",
    "            else:\n",
    "                chosen_combination=random.choice(list(combinations(list(absent_scores.keys()),2)))\n",
    "                si=chosen_combination[0]\n",
    "                sj=chosen_combination[1]\n",
    "                if scores[si]>scores[sj]:\n",
    "                    sentence_to_be_added=si\n",
    "                else:\n",
    "                    sentence_to_be_added=sj\n",
    "                \n",
    "                x[sentence_to_be_added]=1\n",
    "                absent_scores.pop(sentence_to_be_added)\n",
    "                cur_length_of_summary += doc_info.sentence_lengths[sentence_to_be_added]\n",
    "        return (x,True)\n",
    "\n",
    "    elif cur_length_of_summary >=lower_bound and cur_length_of_summary> upper_bound:\n",
    "\n",
    "        while cur_length_of_summary >L+epsilon: #             present_scores = dict(sorted(present_scores.items(), key=lambda item: item[1]))\n",
    "            \n",
    "            if len(list(present_scores.keys()))<=1:\n",
    "                return (x,True)\n",
    "            else:\n",
    "                chosen_combination= random.choice(list(combinations(list(present_scores.keys()),2)))\n",
    "                si=chosen_combination[0]\n",
    "                sj=chosen_combination[1]\n",
    "                if scores[si]> scores[sj]:\n",
    "                    sentence_to_be_removed=sj\n",
    "                else:\n",
    "                    sentence_to_be_removed=si\n",
    "    #             sentence_to_be_removed=list(present_scores.keys())[0]\n",
    "                x[sentence_to_be_removed]=0\n",
    "                present_scores.pop(sentence_to_be_removed)\n",
    "                cur_length_of_summary-=doc_info.sentence_lengths[sentence_to_be_removed]\n",
    "\n",
    "        return (x,True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.101664Z",
     "iopub.status.idle": "2024-03-17T14:37:07.102229Z",
     "shell.execute_reply": "2024-03-17T14:37:07.102044Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.102024Z"
    }
   },
   "outputs": [],
   "source": [
    "def words_per_sentence(list_of_sentences):\n",
    "    words_per_sentence_dict={}\n",
    "    for i in range(len(list_of_sentences)):\n",
    "        words_per_sentence_dict[i]=len(list_of_sentences[i].split())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.103341Z",
     "iopub.status.idle": "2024-03-17T14:37:07.103898Z",
     "shell.execute_reply": "2024-03-17T14:37:07.103726Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.103709Z"
    }
   },
   "outputs": [],
   "source": [
    "def dominate(vec0,vec1):\n",
    "    if vec0==vec1:\n",
    "        return \"False\"\n",
    "    no_of_dimensions=len(vec0)\n",
    "    count_vec=[0]*2\n",
    "\n",
    "    for d in range(no_of_dimensions):\n",
    "        if vec0[d]>vec1[d]:\n",
    "            count_vec[0]+=1\n",
    "        if vec1[d]>vec0[d]:\n",
    "            count_vec[1]+=1\n",
    "    \n",
    "    if count_vec[0]==no_of_dimensions:\n",
    "        #Vec 0 won\n",
    "        return 0\n",
    "    elif count_vec[1]==no_of_dimensions:\n",
    "        #Vec 1 won\n",
    "        return 1\n",
    "    elif count_vec[0]==count_vec[1]:\n",
    "        #no one won= NDS\n",
    "        return \"False\"\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.104715Z",
     "iopub.status.idle": "2024-03-17T14:37:07.105051Z",
     "shell.execute_reply": "2024-03-17T14:37:07.104897Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.104883Z"
    }
   },
   "outputs": [],
   "source": [
    "class member:\n",
    "    def __init__(self,mem_input,doc_info):\n",
    "        self.mem=mem_input.copy()\n",
    "        self.trials=0\n",
    "        self.objective=self.calculate_objective(self.mem,doc_info)\n",
    "        self.fitness=self.transform_objective(self.objective)\n",
    "        self.length_of_summary=self.calculate_length_of_summary(self.mem,doc_info)\n",
    "    \n",
    "    def calculate_length_of_summary(self,x,doc_info):\n",
    "        sum_len=0\n",
    "        for i in range(len(x)):\n",
    "            if x[i]==1:\n",
    "                sum_len+= doc_info.sentence_lengths[i]\n",
    "        return sum_len\n",
    "                \n",
    "        \n",
    "    def transform_objective(self,objective):\n",
    "        return objective\n",
    "    \n",
    "\n",
    "    def calculate_objective(self,mem,doc_info):\n",
    "        if doc_info.topic_enabled==\"v1\":\n",
    "            coverage_obj=coverage_topic(mem,doc_info)\n",
    "            redundancy_obj=redundancy_reduction_topic(mem,doc_info)\n",
    "        elif doc_info.topic_enabled==\"v2\":\n",
    "            coverage_obj=coverage_topic(mem,doc_info)\n",
    "            redundancy_obj=redundancy_reduction(mem,doc_info)\n",
    "        elif doc_info.topic_enabled==\"v3\":\n",
    "            coverage_obj=coverage(mem,doc_info)\n",
    "            redundancy_obj=redundancy_reduction(mem,doc_info)\n",
    "        elif doc_info.topic_enabled==\"v4\":\n",
    "            coverage_obj=coverage(mem,doc_info)\n",
    "            redundancy_obj=redundancy_reduction_topic(mem,doc_info)\n",
    "        return [coverage_obj,redundancy_obj]\n",
    "\n",
    "    \n",
    "def coverage_topic(x,doc_info):\n",
    "    coverage_value=0\n",
    "    for i in range(len(x)):\n",
    "        coverage_value += doc_info.t_o_wik_sim[i] * x[i]\n",
    "    return coverage_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.106358Z",
     "iopub.status.idle": "2024-03-17T14:37:07.106693Z",
     "shell.execute_reply": "2024-03-17T14:37:07.106530Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.106517Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_population(doc_info,less_ct=-1):\n",
    "    list_of_members=[]\n",
    "    if less_ct==-1:\n",
    "        for i in range(doc_info.pop_size):\n",
    "            new_mem=np.random.randint(0,2,doc_info.no_of_sentences)\n",
    "            list_of_members.append(member(new_mem,doc_info))\n",
    "    else:\n",
    "        for i in range(less_ct):\n",
    "            new_mem=np.random.randint(0,2,doc_info.no_of_sentences)\n",
    "            list_of_members.append(member(new_mem,doc_info))\n",
    "    return list_of_members\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.108677Z",
     "iopub.status.idle": "2024-03-17T14:37:07.109011Z",
     "shell.execute_reply": "2024-03-17T14:37:07.108862Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.108849Z"
    }
   },
   "outputs": [],
   "source": [
    "def naive_replace(list_of_members,current_mem,new_mem,i):\n",
    "    new_mem.trials=0\n",
    "    list_of_members[i]=new_mem\n",
    "    return list_of_members\n",
    "def replace(list_of_members,current_mem,new_mem,i):\n",
    "    result=dominate(current_mem.fitness,new_mem.fitness)\n",
    "    if result==0:\n",
    "        print(\"Not\",current_mem.fitness,new_mem.fitness)\n",
    "        list_of_members[i].trials+=1\n",
    "    elif result==1:\n",
    "        print(\"WON\",current_mem.fitness,new_mem.fitness)\n",
    "        new_mem.trials=0\n",
    "        list_of_members[i]=new_mem\n",
    "    elif result==\"False\":\n",
    "        #print(\"Non Dominating each other\",current_mem.fitness,new_mem.fitness)\n",
    "        list_of_members[i].trials+=1\n",
    "    return list_of_members\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def send_employed_bees(list_of_members, doc_info):\n",
    "    start_time = time.time()  # Start timing the function\n",
    "    no_of_members = len(list_of_members)\n",
    "    l = []\n",
    "    for i in range(no_of_members):\n",
    "        current_mem = list_of_members[i]\n",
    "#         time_before_mutation = time.time()\n",
    "        new_mem_list = mutation_operator_select(current_mem.mem.copy(), doc_info)\n",
    "#         time_after_mutation = time.time()\n",
    "        new_mem = member(new_mem_list.copy(), doc_info)\n",
    "#         time_after_member_creation = time.time()\n",
    "        \n",
    "        \n",
    "        list_of_members=replacement_operator_select(list_of_members,current_mem,new_mem,i,doc_info)\n",
    "#         time_after_replace = time.time()\n",
    "        \n",
    "#         print(\"Time for mutation:\", time_after_mutation - time_before_mutation)\n",
    "#         print(\"Time for member creation:\", time_after_member_creation - time_after_mutation)\n",
    "#         print(\"Time for replace\", time_after_replace - time_after_member_creation)\n",
    "\n",
    "    return list_of_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.109851Z",
     "iopub.status.idle": "2024-03-17T14:37:07.110167Z",
     "shell.execute_reply": "2024-03-17T14:37:07.110017Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.110004Z"
    }
   },
   "outputs": [],
   "source": [
    "def roulette_wheel_selection(probabilities):\n",
    "    cumulative_probabilities = [sum(probabilities[:i+1]) for i in range(len(probabilities))]\n",
    "    total_probability = cumulative_probabilities[-1]\n",
    "    random_number = random.uniform(0, total_probability)\n",
    "    for i, cumulative_prob in enumerate(cumulative_probabilities):\n",
    "        if random_number <= cumulative_prob:\n",
    "            return i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.111436Z",
     "iopub.status.idle": "2024-03-17T14:37:07.111780Z",
     "shell.execute_reply": "2024-03-17T14:37:07.111623Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.111609Z"
    }
   },
   "outputs": [],
   "source": [
    "def ranking(list_of_members):\n",
    "    sp_dict={}\n",
    "    np_dict={}\n",
    "    f_dict={}\n",
    "    f_dict[1]=[]\n",
    "    for i in range(len(list_of_members)):\n",
    "        p=list_of_members[i]\n",
    "        sp_dict[i]=[]\n",
    "        np_dict[i]=0\n",
    "        for j in range(len(list_of_members)):\n",
    "            if i!=j:\n",
    "                q=list_of_members[j]\n",
    "                result=dominate(p.objective,q.objective)\n",
    "                if result==0:\n",
    "                    sp_dict[i].append(j)\n",
    "                elif result==1:\n",
    "                    np_dict[i]+=1\n",
    "        if np_dict[i]==0:\n",
    "            f_dict[1].append(i)\n",
    "            \n",
    "    \n",
    "    front_index=1\n",
    "    while len(f_dict[front_index])!=0:\n",
    "        Q=[]\n",
    "        for p in f_dict[front_index]:\n",
    "            for q in sp_dict[p]:\n",
    "                np_dict[q]-=1\n",
    "                if np_dict[q]==0:\n",
    "                    Q.append(q)\n",
    "        \n",
    "        front_index+=1\n",
    "        f_dict[front_index]=Q.copy()\n",
    "    \n",
    "    if len(f_dict[front_index])==0:\n",
    "        r_v=f_dict.pop(front_index)\n",
    "    \n",
    "    return f_dict.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.112984Z",
     "iopub.status.idle": "2024-03-17T14:37:07.113317Z",
     "shell.execute_reply": "2024-03-17T14:37:07.113161Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.113147Z"
    }
   },
   "outputs": [],
   "source": [
    "def sort_dict(my_dict,objective_index):\n",
    "    sorted_dict = dict(sorted(my_dict.items(), key=lambda item: item[1][objective_index]))\n",
    "    return sorted_dict\n",
    "\n",
    "def crowding_distance(front_member_indices,list_of_members,no_of_objectives=2):\n",
    "    current_member_objective_dict={}\n",
    "    di_dict={}\n",
    "    for member_index in front_member_indices:\n",
    "        current_member_objective_dict[member_index]=list_of_members[member_index].objective\n",
    "        di_dict[member_index]=10**-8\n",
    "        \n",
    "    for objective_index in range(no_of_objectives):\n",
    "        sorted_dict_list=list(sort_dict(current_member_objective_dict.copy(),objective_index))\n",
    "        if len(sorted_dict_list)<=2:\n",
    "            for i in range(len(sorted_dict_list)):\n",
    "                di_dict[sorted_dict_list[i]]=10**8\n",
    "            return di_dict\n",
    "        else:\n",
    "            lower_extreme=sorted_dict_list[0]\n",
    "            upper_extreme=sorted_dict_list[-1]\n",
    "            di_dict[lower_extreme]=10**8\n",
    "            di_dict[upper_extreme]=10**8\n",
    "            for i in range(2,len(sorted_dict_list)-1):\n",
    "                left_index=sorted_dict_list[i-1]\n",
    "                right_index=sorted_dict_list[i+1]\n",
    "                right_obj=list_of_members[right_index].objective[objective_index]\n",
    "                left_obj=list_of_members[left_index].objective[objective_index]\n",
    "                upper_obj=list_of_members[upper_extreme].objective[objective_index]\n",
    "                lower_obj=list_of_members[lower_extreme].objective[objective_index]\n",
    "                #print(list_of_members[right_index].objective[objective_index],list_of_members[left_index].objective[objective_index],list_of_members[upper_extreme].objective[objective_index],list_of_members[lower_extreme].objective[objective_index])\n",
    "                val=abs(right_obj-left_obj)/abs(upper_obj-lower_obj)\n",
    "                if val>=0 or val<0:\n",
    "                    di_dict[sorted_dict_list[i]]+=val\n",
    "                else:\n",
    "#                     print(list_of_members[right_index].objective[objective_index],list_of_members[left_index].objective[objective_index],list_of_members[upper_extreme].objective[objective_index],list_of_members[lower_extreme].objective[objective_index])\n",
    "#                     print(\"OMG THIS HAS HAPPENED\")\n",
    "                    di_dict[sorted_dict_list[i]]+=1\n",
    "    return di_dict\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.115472Z",
     "iopub.status.idle": "2024-03-17T14:37:07.115976Z",
     "shell.execute_reply": "2024-03-17T14:37:07.115745Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.115724Z"
    }
   },
   "outputs": [],
   "source": [
    "class front_info:\n",
    "    def __init__(self,member,member_index,rank,cd):\n",
    "        self.member_index=member_index\n",
    "        self.member=member\n",
    "        self.rank=rank\n",
    "        self.cd=cd\n",
    "\n",
    "def probability_calculations(front_info_list):\n",
    "    prob_list=[]\n",
    "    for i in range(len(front_info_list)):\n",
    "        value=1/(front_info_list[i].rank +(1/(1+front_info_list[i].cd)))\n",
    "        prob_list.append(0.9*value +0.1)\n",
    "    \n",
    "    return prob_list \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.116890Z",
     "iopub.status.idle": "2024-03-17T14:37:07.117364Z",
     "shell.execute_reply": "2024-03-17T14:37:07.117143Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.117124Z"
    }
   },
   "outputs": [],
   "source": [
    "def front_info_former(list_of_members,doc_info):\n",
    "    no_of_objectives=doc_info.no_of_objectives\n",
    "    f_dict=ranking(list_of_members)\n",
    "#     for k,v in f_dict.items():\n",
    "#         print(\"!!!!! Rank = \",k)\n",
    "#         for me in v:\n",
    "#             print(list_of_members[me].objective)\n",
    "            \n",
    "    front_info_dict={}\n",
    "    for i in range(len(list_of_members)):\n",
    "        front_info_dict[i]=0\n",
    "    for k,v in f_dict.items():\n",
    "        cd_values=crowding_distance(v,list_of_members,no_of_objectives)\n",
    "        for member_index in v:\n",
    "            front_info_dict[member_index]=front_info(list_of_members[member_index],member_index,k,cd_values[member_index])\n",
    "    return front_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.118872Z",
     "iopub.status.idle": "2024-03-17T14:37:07.119355Z",
     "shell.execute_reply": "2024-03-17T14:37:07.119128Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.119108Z"
    }
   },
   "outputs": [],
   "source": [
    "def send_onlooker_bees(list_of_members,doc_info):\n",
    "    no_of_objectives=doc_info.no_of_objectives\n",
    "    front_info_list=list(front_info_former(list_of_members,doc_info).values())\n",
    "\n",
    "    prob_list=probability_calculations(front_info_list)\n",
    "    no_of_members=len(list_of_members)\n",
    "    l=[]\n",
    "    for i in range(no_of_members):\n",
    "        roulette_index=roulette_wheel_selection(prob_list)\n",
    "        current_mem=list_of_members[roulette_index]\n",
    "        new_mem_list=mutation_operator_select(current_mem.mem.copy(),doc_info)\n",
    "        new_mem=member(new_mem_list.copy(),doc_info)\n",
    "        result=dominate(current_mem.objective, new_mem.objective)\n",
    "        if result==0 or result==\"False\":\n",
    "            list_of_members[roulette_index].trials+=1\n",
    "        list_of_members.append(new_mem)\n",
    "    return list_of_members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.120724Z",
     "iopub.status.idle": "2024-03-17T14:37:07.121211Z",
     "shell.execute_reply": "2024-03-17T14:37:07.120974Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.120954Z"
    }
   },
   "outputs": [],
   "source": [
    "def rank_and_crowd_onlookers(list_of_members,doc_info):\n",
    "    no_of_objectives=doc_info.no_of_objectives\n",
    "    front_info_list=list(front_info_former(list_of_members,doc_info).values())\n",
    "    prob_list=probability_calculations(front_info_list)\n",
    "    prob_dict = {index: value for index, value in enumerate(prob_list)}\n",
    "    sorted_dict = dict(sorted(prob_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    ranked_list_indices=list(sorted_dict.keys())[0:int(len(list_of_members)/2)]\n",
    "    ranked_list_of_members=[]\n",
    "    for key in ranked_list_indices:\n",
    "        ranked_list_of_members.append(list_of_members[key])\n",
    "    return ranked_list_of_members\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.122644Z",
     "iopub.status.idle": "2024-03-17T14:37:07.123142Z",
     "shell.execute_reply": "2024-03-17T14:37:07.122899Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.122879Z"
    }
   },
   "outputs": [],
   "source": [
    "def send_scout_bees(list_of_members,doc_info):\n",
    "    for i in range(len(list_of_members)):\n",
    "        current_mem=list_of_members[i]\n",
    "        if current_mem.trials<=doc_info.trial_cutoff_limit:\n",
    "            pass\n",
    "        else:\n",
    "            soln_beaten=False\n",
    "            for mutation_trial in range(10): #range(cycle)\n",
    "                if soln_beaten==True:\n",
    "                    break\n",
    "                else:\n",
    "                    new_mem_list=mutation_operator_select(current_mem.mem.copy(),doc_info)\n",
    "                    new_mem=member(new_mem_list.copy(),doc_info)\n",
    "                    result=dominate(current_mem.objective,new_mem.objective)\n",
    "                    if result==1:\n",
    "                        list_of_members=naive_replace(list_of_members,current_mem,new_mem,i)\n",
    "                        soln_beaten=True\n",
    "            \n",
    "            if soln_beaten==False:\n",
    "                #Soln still not replaced\n",
    "                random_selected_member=member(np.random.randint(0,2,len(list_of_members[0].mem)),doc_info)\n",
    "                random_selected_member.trials=0\n",
    "                list_of_members=naive_replace(list_of_members,current_mem,random_selected_member,i)\n",
    "    \n",
    "    return list_of_members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.125151Z",
     "iopub.status.idle": "2024-03-17T14:37:07.125657Z",
     "shell.execute_reply": "2024-03-17T14:37:07.125412Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.125393Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def NDS_insert_ver_5(NDS_archive,list_of_members):\n",
    "    for i in range(len(list_of_members)):\n",
    "        member_to_insert=list_of_members[i]\n",
    "        if len(NDS_archive)==0:\n",
    "            NDS_archive.append(member_to_insert)\n",
    "        else:\n",
    "            to_be_removed=[]\n",
    "            adding_flag=True\n",
    "            for j in range(len(NDS_archive)):\n",
    "                result=dominate(member_to_insert.objective,NDS_archive[j].objective)\n",
    "                if result==0:\n",
    "                    #This means ES has won!\n",
    "                    to_be_removed.append(j)\n",
    "                elif result==1:\n",
    "                    #This means NDS archive item has won\n",
    "                    adding_flag=False\n",
    "            new_NDS_archive=[]\n",
    "            for ind in range(len(NDS_archive)):\n",
    "                #Here whatever items are present in to_be_removed list, are not added to new NDS archive\n",
    "                if ind not in to_be_removed:\n",
    "                    new_NDS_archive.append(NDS_archive[ind])\n",
    "            \n",
    "            if adding_flag==True:\n",
    "                # If none of the NDS archive items were able to dominate ES, then we insert it\n",
    "                new_NDS_archive.append(member_to_insert)\n",
    "            NDS_archive=new_NDS_archive.copy()\n",
    "                \n",
    "    return NDS_archive\n",
    "                \n",
    "                \n",
    "def size_check(x,list_of_sentences):\n",
    "    len_of_summary=0\n",
    "    for i in range(len(x)):\n",
    "        if x[i]==1:\n",
    "            len_of_summary+=len(list_of_sentences[i].split())\n",
    "    return len_of_summary\n",
    "\n",
    "\n",
    "def NDS_insert_ver_6(NDS_archive,list_of_members,doc_info):\n",
    "    list_of_sentences=doc_info.list_of_sentences\n",
    "    L=doc_info.L\n",
    "    epsilon=doc_info.L\n",
    "    for i in range(len(list_of_members)):\n",
    "        member_to_insert=list_of_members[i]\n",
    "        if len(NDS_archive)==0:\n",
    "            NDS_archive.append(member_to_insert)\n",
    "        else:\n",
    "            to_be_removed=[]\n",
    "            adding_flag=True\n",
    "            for j in range(len(NDS_archive)):\n",
    "                result=dominate(member_to_insert.objective,NDS_archive[j].objective)\n",
    "                if result==0:\n",
    "                    #This means ES has won!\n",
    "                    to_be_removed.append(j)\n",
    "                elif result==1:\n",
    "                    #This means NDS archive item has won\n",
    "                    adding_flag=False\n",
    "                elif member_to_insert.objective==NDS_archive[j].objective:\n",
    "                    adding_flag=False\n",
    "#                     print(\"Ram Ram\")\n",
    "            new_NDS_archive=[]\n",
    "            for ind in range(len(NDS_archive)):\n",
    "                #Here whatever items are present in to_be_removed list, are not added to new NDS archive\n",
    "                if ind not in to_be_removed:\n",
    "                    len_of_summary=size_check(NDS_archive[ind].mem,list_of_sentences)\n",
    "                    if len_of_summary >=L-epsilon and len_of_summary <= L+epsilon:\n",
    "                        new_NDS_archive.append(NDS_archive[ind])\n",
    "            \n",
    "            if adding_flag==True:\n",
    "                # If none of the NDS archive items were able to dominate ES, then we insert it\n",
    "                len_of_summary=size_check(member_to_insert.mem,list_of_sentences)\n",
    "                if len_of_summary <= L+epsilon:\n",
    "                    new_NDS_archive.append(member_to_insert)\n",
    "            NDS_archive=new_NDS_archive.copy()\n",
    "                \n",
    "    return NDS_archive     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.127374Z",
     "iopub.status.idle": "2024-03-17T14:37:07.127869Z",
     "shell.execute_reply": "2024-03-17T14:37:07.127641Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.127621Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(NDS_archive):\n",
    "    normalized_NDS_archive=copy.deepcopy(NDS_archive)\n",
    "\n",
    "    min_obj0=float('inf')\n",
    "    max_obj0=0\n",
    "    \n",
    "    min_obj1=float('inf')\n",
    "    max_obj1=0\n",
    "    \n",
    "    for soln in NDS_archive:\n",
    "        min_obj0=min(min_obj0,soln.objective[0])\n",
    "        max_obj0=max(max_obj0,soln.objective[0])\n",
    "        min_obj1=min(min_obj1,soln.objective[1])\n",
    "        max_obj1=max(max_obj1,soln.objective[1])\n",
    "    \n",
    "\n",
    "    for i in range(len(NDS_archive)):\n",
    "        soln=NDS_archive[i]\n",
    "        normalized_obj0 = (soln.objective[0] - min_obj0) / (max_obj0 - min_obj0)\n",
    "        normalized_obj1 = (soln.objective[1] - min_obj1) / (max_obj1 - min_obj1)\n",
    "        normalized_NDS_archive[i].objective[0]=normalized_obj0\n",
    "        normalized_NDS_archive[i].objective[1]=normalized_obj1\n",
    "    return normalized_NDS_archive.copy()\n",
    "\n",
    "def mahalakshmi(p1,p2,sigma_dict):\n",
    "    value=0\n",
    "    for i in range(len(sigma_dict.values())):\n",
    "        cod=i #current objective dimension\n",
    "        value += ((p1[cod] - p2[cod])/sigma_dict[cod])**2\n",
    "    value=np.sqrt(value)\n",
    "    return value\n",
    "\n",
    "def standard_deviation(normalized_NDS_archive):\n",
    "    no_of_objectives=len(normalized_NDS_archive[0].objective)\n",
    "    obj_dict={}\n",
    "    sigma_dict={}\n",
    "    mean_dict={}\n",
    "    for obj_index in range(no_of_objectives):\n",
    "        obj_dict[obj_index]=[]\n",
    "        sigma_dict[obj_index]=0\n",
    "        mean_dict[obj_index]=0\n",
    "    \n",
    "    for i in range(len(normalized_NDS_archive)):\n",
    "        for obj_index in range(no_of_objectives):\n",
    "            obj_dict[obj_index].append(normalized_NDS_archive[i].objective[obj_index])\n",
    "            \n",
    "    for obj_index in range(no_of_objectives):\n",
    "        mean_dict[obj_index]=np.mean(obj_dict[obj_index])\n",
    "    for obj_index in range(no_of_objectives):\n",
    "        for v in obj_dict[obj_index]:\n",
    "            sigma_dict[obj_index]+=(v-mean_dict[obj_index])**2\n",
    "        sigma_dict[obj_index]/=len(normalized_NDS_archive)\n",
    "    \n",
    "    return sigma_dict\n",
    "\n",
    "def shortest_point_to_ideal_point_mahalanobis(NDS_archive,doc_info):\n",
    "    NNA=normalize(NDS_archive)\n",
    "    sigma_dict=standard_deviation(NNA)\n",
    "    best_point=NNA[0]\n",
    "    ideal_point=(1,1)\n",
    "    shortest_distance=mahalakshmi(best_point.objective,ideal_point,sigma_dict)\n",
    "    for soln in range(1,len(NNA)):\n",
    "        calc_dist=mahalakshmi(NNA[soln].objective,ideal_point,sigma_dict)\n",
    "        if shortest_distance > calc_dist :\n",
    "            best_point=NNA[soln]\n",
    "            shortest_distance=calc_dist\n",
    "    return best_point\n",
    "\n",
    "\n",
    "\n",
    "def euclidean(vector1, vector2):\n",
    "    if len(vector1) != len(vector2):\n",
    "        raise ValueError(\"Vectors must have the same dimensionality\")\n",
    "    \n",
    "    sum_of_squares = sum((x - y) ** 2 for x, y in zip(vector1, vector2))\n",
    "    return math.sqrt(sum_of_squares)\n",
    "\n",
    "\n",
    "def shortest_point_to_ideal_point_euclidean(NDS_archive,doc_info):\n",
    "    NNA=normalize(NDS_archive)\n",
    "    best_point=NNA[0]\n",
    "    ideal_point=(1,1)\n",
    "    shortest_distance=euclidean(best_point.objective,ideal_point)\n",
    "    for soln in range(1,len(NNA)):\n",
    "        calc_dist=euclidean(NNA[soln].objective,ideal_point)\n",
    "        if shortest_distance > calc_dist :\n",
    "            best_point=NNA[soln]\n",
    "            shortest_distance=calc_dist\n",
    "    return best_point\n",
    "def manhattan(vector1, vector2):\n",
    "    if len(vector1) != len(vector2):\n",
    "        raise ValueError(\"Vectors must have the same dimensionality\")\n",
    "    \n",
    "    return sum(abs(x - y) for x, y in zip(vector1, vector2))\n",
    "\n",
    "def shortest_point_to_ideal_point_manhattan(NDS_archive,doc_info):\n",
    "    NNA=normalize(NDS_archive)\n",
    "    best_point=NNA[0]\n",
    "    ideal_point=(1,1)\n",
    "    shortest_distance=manhattan(best_point.objective,ideal_point)\n",
    "    for soln in range(1,len(NNA)):\n",
    "        calc_dist=manhattan(NNA[soln].objective,ideal_point)\n",
    "        if shortest_distance > calc_dist :\n",
    "            best_point=NNA[soln]\n",
    "            shortest_distance=calc_dist\n",
    "    return best_point\n",
    "\n",
    "\n",
    "def chebyshev(vector1, vector2):\n",
    "    if len(vector1) != len(vector2):\n",
    "        raise ValueError(\"Vectors must have the same dimensionality\")\n",
    "        \n",
    "    return max(abs(x - y) for x, y in zip(vector1, vector2))\n",
    "\n",
    "def shortest_point_to_ideal_point_chebyshev(NDS_archive,doc_info):\n",
    "    NNA=normalize(NDS_archive)\n",
    "    best_point=NNA[0]\n",
    "    ideal_point=(1,1)\n",
    "    shortest_distance=chebyshev(best_point.objective,ideal_point)\n",
    "    for soln in range(1,len(NNA)):\n",
    "        calc_dist=chebyshev(NNA[soln].objective,ideal_point)\n",
    "        if shortest_distance > calc_dist :\n",
    "            best_point=NNA[soln]\n",
    "            shortest_distance=calc_dist\n",
    "    return best_point\n",
    "\n",
    "def calculate_all_points(NDS_archive,origin_vec,doc_info,distance_type):\n",
    "    val=0\n",
    "    sigma_dict=standard_deviation(NDS_archive)\n",
    "    for item in NDS_archive:\n",
    "        if distance_type==\"chebyshev\":\n",
    "            val +=chebyshev(origin_vec,item.objective)\n",
    "        elif distance_type==\"manhattan\":\n",
    "            val +=manhattan(origin_vec,item.objective)\n",
    "        elif distance_type==\"euclidean\":\n",
    "            val +=euclidean(origin_vec,item.objective)\n",
    "        elif distance_type==\"mahalanobis\":\n",
    "            val +=mahalakshmi(origin_vec,item.objective,sigma_dict)\n",
    "    return val\n",
    "            \n",
    "def shortest_point_to_all_points_chebyshev(NDS_archive,doc_info):\n",
    "    best_point=NDS_archive[0]\n",
    "    shortest_distance=calculate_all_points(NDS_archive,best_point.objective,doc_info,\"chebyshev\")\n",
    "    for soln in range(1,len(NDS_archive)):\n",
    "        calc_dist=calculate_all_points(NDS_archive,NDS_archive[soln].objective,doc_info,\"chebyshev\")\n",
    "        if shortest_distance > calc_dist :\n",
    "            best_point=NDS_archive[soln]\n",
    "            shortest_distance=calc_dist\n",
    "    return best_point\n",
    "\n",
    "def shortest_point_to_all_points_manhattan(NDS_archive,doc_info):\n",
    "    best_point=NDS_archive[0]\n",
    "    shortest_distance=calculate_all_points(NDS_archive,best_point.objective,doc_info,\"manhattan\")\n",
    "    for soln in range(1,len(NDS_archive)):\n",
    "        calc_dist=calculate_all_points(NDS_archive,NDS_archive[soln].objective,doc_info,\"manhattan\")\n",
    "        if shortest_distance > calc_dist :\n",
    "            best_point=NDS_archive[soln]\n",
    "            shortest_distance=calc_dist\n",
    "    return best_point\n",
    "\n",
    "def shortest_point_to_all_points_euclidean(NDS_archive,doc_info):\n",
    "    best_point=NDS_archive[0]\n",
    "    shortest_distance=calculate_all_points(NDS_archive,best_point.objective,doc_info,\"euclidean\")\n",
    "    for soln in range(1,len(NDS_archive)):\n",
    "        calc_dist=calculate_all_points(NDS_archive,NDS_archive[soln].objective,doc_info,\"euclidean\")\n",
    "        if shortest_distance > calc_dist :\n",
    "            best_point=NDS_archive[soln]\n",
    "            shortest_distance=calc_dist\n",
    "    return best_point\n",
    "\n",
    "def shortest_point_to_all_points_mahalanobis(NDS_archive,doc_info):\n",
    "    best_point=NDS_archive[0]\n",
    "    shortest_distance=calculate_all_points(NDS_archive,best_point.objective,doc_info,\"mahalanobis\")\n",
    "    for soln in range(1,len(NDS_archive)):\n",
    "        calc_dist=calculate_all_points(NDS_archive,NDS_archive[soln].objective,doc_info,\"mahalanobis\")\n",
    "        if shortest_distance > calc_dist :\n",
    "            best_point=NDS_archive[soln]\n",
    "            shortest_distance=calc_dist\n",
    "    return best_point\n",
    "\n",
    "def topmost_coverage(NDS_archive,doc_info):\n",
    "    list_of_sentences=doc_info.list_of_sentences\n",
    "    Lmax=doc_info.L + doc_info.epsilon\n",
    "    best_mem=-1\n",
    "    best_mem_score=-1\n",
    "    for item in NDS_archive:\n",
    "        if item.objective[0]>best_mem_score:\n",
    "            best_mem=item\n",
    "            best_mem_score=item.objective[0]\n",
    "    return best_mem\n",
    "\n",
    "def topmost_redundancy_reduction(NDS_archive,doc_info):\n",
    "    list_of_sentences=doc_info.list_of_sentences\n",
    "    Lmax=doc_info.L + doc_info.epsilon\n",
    "    best_mem=-1\n",
    "    best_mem_score=-1\n",
    "    for item in NDS_archive:\n",
    "        if item.objective[1]>best_mem_score:\n",
    "            best_mem=item\n",
    "            best_mem_score=item.objective[1]\n",
    "    return best_mem\n",
    "\n",
    "def consensus(NDS_archive,doc_info):\n",
    "    list_of_sentences=doc_info.list_of_sentences\n",
    "    Lmax=doc_info.L + doc_info.epsilon\n",
    "    sentence_count_dict={}\n",
    "    no_of_sentences=len(NDS_archive[0].mem)\n",
    "    for i in range(no_of_sentences):\n",
    "        sentence_count_dict[i]=0\n",
    "    for i in range(len(NDS_archive)):\n",
    "        for j in range(no_of_sentences):\n",
    "            if NDS_archive[i].mem[j]==1:\n",
    "                sentence_count_dict[j]+=1\n",
    "    \n",
    "    sentence_count_dict_sorted = dict(sorted(sentence_count_dict.items(), key=lambda item: item[1],reverse=True))\n",
    "    final_mem=[0]*no_of_sentences\n",
    "    cur_length=0\n",
    "    for k,v in sentence_count_dict_sorted.items():\n",
    "        if cur_length + doc_info.sentence_lengths[k] >Lmax:\n",
    "            break\n",
    "        else:\n",
    "            final_mem[k]=1\n",
    "            cur_length+=doc_info.sentence_lengths[k]\n",
    "#     print(sentence_count_dict_sorted)\n",
    "    return member(final_mem,doc_info)\n",
    "        \n",
    "\n",
    "def largest_hypervolume(NDS_archive,doc_info):\n",
    "    l_area=-1\n",
    "    l_mem=0\n",
    "    \n",
    "    for archive_member in NDS_archive:\n",
    "        cur_area=abs(archive_member.objective[0] * archive_member.objective[1])\n",
    "        if cur_area > l_area:\n",
    "            l_area=cur_area\n",
    "            l_mem=archive_member\n",
    "    return l_mem\n",
    "\n",
    "def calculate_all_points_levenshtein(NDS_archive,sumi):\n",
    "    sumi_set=set([i for i, val in enumerate(sumi) if val == 1])\n",
    "    dist=0\n",
    "    for item in NDS_archive:\n",
    "        sumj_set=set([i for i,val in enumerate(item.mem) if val==1])\n",
    "        dist+=len(sumi_set) + len(sumj_set) - 2*len(sumi_set.intersection(sumj_set))\n",
    "    return dist\n",
    "        \n",
    "    \n",
    "def shortest_point_to_all_points_levenshtein(NDS_archive,doc_info):\n",
    "    best_point=NDS_archive[0]\n",
    "    shortest_distance=calculate_all_points_levenshtein(NDS_archive,best_point.mem)\n",
    "    for soln in range(1,len(NDS_archive)):\n",
    "        calc_dist=calculate_all_points_levenshtein(NDS_archive,NDS_archive[soln].mem)\n",
    "        if shortest_distance > calc_dist :\n",
    "            best_point=NDS_archive[soln]\n",
    "            shortest_distance=calc_dist\n",
    "    return best_point\n",
    "\n",
    "class rouge_score_details:\n",
    "    def __init__(self,NDS_archive,doc_info):\n",
    "        self.consensus_extract=consensus(NDS_archive,doc_info)\n",
    "        self.largest_hypervolume_extract=largest_hypervolume(NDS_archive,doc_info)\n",
    "        self.topmost_coverage_extract=topmost_coverage(NDS_archive,doc_info)\n",
    "        self.topmost_redundancy_reduction_extract=topmost_redundancy_reduction(NDS_archive,doc_info)\n",
    "        self.shortest_point_to_ideal_point_euclidean_extract=shortest_point_to_ideal_point_euclidean(NDS_archive,doc_info)\n",
    "        self.shortest_point_to_ideal_point_manhattan_extract=shortest_point_to_ideal_point_manhattan(NDS_archive,doc_info)\n",
    "        self.shortest_point_to_ideal_point_chebyshev_extract=shortest_point_to_ideal_point_chebyshev(NDS_archive,doc_info)\n",
    "        self.shortest_point_to_ideal_point_mahalanobis_extract=shortest_point_to_ideal_point_mahalanobis(NDS_archive,doc_info)\n",
    "        self.shortest_point_to_all_points_chebyshev_extract=shortest_point_to_all_points_chebyshev(NDS_archive,doc_info)\n",
    "        self.shortest_point_to_all_points_manhattan_extract=shortest_point_to_all_points_manhattan(NDS_archive,doc_info)\n",
    "        self.shortest_point_to_all_points_mahalanobis_extract=shortest_point_to_all_points_mahalanobis(NDS_archive,doc_info)\n",
    "        self.shortest_point_to_all_points_euclidean_extract=shortest_point_to_all_points_euclidean(NDS_archive,doc_info)\n",
    "        self.shortest_point_to_all_points_levenshtein_extract=shortest_point_to_all_points_levenshtein(NDS_archive,doc_info)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.129061Z",
     "iopub.status.idle": "2024-03-17T14:37:07.129544Z",
     "shell.execute_reply": "2024-03-17T14:37:07.129321Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.129301Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_objectives(NDS_archive,title_string):\n",
    "    x_values=[]\n",
    "    y_values=[]\n",
    "    \n",
    "    for soln in NDS_archive:\n",
    "        x_values.append(soln.objective[0])\n",
    "        y_values.append(soln.objective[1])\n",
    "    \n",
    "    plt.scatter(x_values, y_values, label='Multi Objective Optimization')\n",
    "    plt.xlabel('Coverage Objective')\n",
    "    plt.ylabel('Redundancy Reduction Objective')\n",
    "    plt.title('Multi Objective : '+title_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.131038Z",
     "iopub.status.idle": "2024-03-17T14:37:07.131524Z",
     "shell.execute_reply": "2024-03-17T14:37:07.131296Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.131276Z"
    }
   },
   "outputs": [],
   "source": [
    "def SBERT_embeddings(list_of_sentences):\n",
    "    o_dict={}\n",
    "    wik_dict={}\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings=model.encode(list_of_sentences[0])\n",
    "    for i in range(len(embeddings)):\n",
    "        o_dict[i]=0\n",
    "#         o_dict[i]=embeddings[i]\n",
    "\n",
    "    for s_index in range(len(list_of_sentences)):\n",
    "        embeddings=model.encode(list_of_sentences[s_index])\n",
    "        wik_dict[s_index]={}\n",
    "        for j in range(len(embeddings)):\n",
    "            wik_dict[s_index][j]=embeddings[j]\n",
    "            o_dict[j]+=embeddings[j]\n",
    "\n",
    "    for j in range(len(embeddings)):\n",
    "        o_dict[j]/=len(list_of_sentences)\n",
    "\n",
    "#     print(o_dict,temp)\n",
    "    return (o_dict,wik_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.133439Z",
     "iopub.status.idle": "2024-03-17T14:37:07.133933Z",
     "shell.execute_reply": "2024-03-17T14:37:07.133705Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.133685Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(preprocessed_inputs):\n",
    "    preprocessed_sentences = []\n",
    "    for i in range(len(preprocessed_inputs)):\n",
    "        preprocessed_sentences.append(\" \".join(preprocessed_inputs[i]))\n",
    "\n",
    "    return preprocessed_sentences\n",
    "\n",
    "    \n",
    "\n",
    "def retrieve_topic_assignment_matrix(list_of_sentences,preprocessed_inputs, num_topics=10,n_top_words=20):\n",
    "\n",
    "#     preprocessed_sentences = preprocess_data(preprocessed_inputs)\n",
    "#     vectorizer = CountVectorizer()\n",
    "#     document_term_matrix = vectorizer.fit_transform(preprocessed_sentences)\n",
    "#     lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "#     lda_output = lda_model.fit_transform(document_term_matrix)\n",
    "#     feature_names = vectorizer.get_feature_names_out()\n",
    "#     topic_to_words_mapper={}\n",
    "#     for topic_idx, topic in enumerate(lda_model.components_):\n",
    "#         top_words_idx = topic.argsort()[:-n_top_words - 1:-1]\n",
    "#         top_words = [feature_names[i] for i in top_words_idx]\n",
    "#         topic_to_words_mapper[topic_idx]=top_words.copy()\n",
    "        \n",
    "\n",
    "#     sentence_topic_probabilities = lda_model.transform(document_term_matrix)\n",
    "\n",
    "    mdl = tp.HDPModel()\n",
    "    for line in preprocessed_inputs:\n",
    "        mdl.add_doc(line)\n",
    "    for i in range(0, 100, 10):\n",
    "        mdl.train(10)\n",
    "    \n",
    "    LDA_model=mdl.convert_to_lda()[0]\n",
    "    \n",
    "    sentence_topic_probabilities=[]\n",
    "    topic_to_words_mapper={}\n",
    "    for i in range(len(preprocessed_inputs)):\n",
    "        sentence_topic_probabilities.append(LDA_model.docs[i].get_topic_dist())\n",
    "    \n",
    "    sentence_topic_probabilities=np.array(sentence_topic_probabilities)\n",
    "    for i in range(mdl.k):\n",
    "        topic_to_words_mapper[i]=[]\n",
    "        for word in mdl.get_topic_words(i):\n",
    "            topic_to_words_mapper[i].append(word)\n",
    "    \n",
    "            \n",
    "        \n",
    "    # Set the threshold probability\n",
    "    threshold_probability = 0.5\n",
    "    \n",
    "    sentences=list_of_sentences\n",
    "    topic_assignment_matrix = np.zeros((len(sentences), mdl.k), dtype=int)\n",
    "    for sentence_idx, sentence_probabilities in enumerate(sentence_topic_probabilities):\n",
    "        top_topic_idx = sentence_probabilities.argmax()\n",
    "        top_topic_prob = sentence_probabilities[top_topic_idx]\n",
    "\n",
    "        topic_assignment = 1 if top_topic_prob > threshold_probability else 0\n",
    "\n",
    "        topic_assignment_matrix[sentence_idx][top_topic_idx] = topic_assignment\n",
    "    return topic_assignment_matrix.copy(),topic_to_words_mapper.copy(),sentence_topic_probabilities.copy(),mdl.k\n",
    "\n",
    "\n",
    "def extract_keywords(text, num_keywords=20):\n",
    "    model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "    keywords = model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words='english', top_n=num_keywords)\n",
    "    return [keyword[0] for keyword in keywords]\n",
    "def create_sentence_keyword_matrix(sentences, keywords):\n",
    "    vectorizer = CountVectorizer(vocabulary=keywords, binary=True)\n",
    "    sentence_matrix = vectorizer.fit_transform(sentences)\n",
    "    keyword_matrix = sentence_matrix  \n",
    "    return keyword_matrix.toarray()\n",
    "def create_binary_matrix(sentences, keywords):\n",
    "    sentence_matrix = create_sentence_keyword_matrix(sentences, keywords)\n",
    "    binary_matrix = np.where(sentence_matrix > 0, 1, 0)\n",
    "    return binary_matrix\n",
    "def retrieve_kw_matrix(revised_text_file_content,list_of_sentences,num_keywords):\n",
    "    document=revised_text_file_content\n",
    "    extracted_keywords = extract_keywords(document, num_keywords)\n",
    "    binary_matrix = create_binary_matrix(list_of_sentences, extracted_keywords)\n",
    "    kw_matrix=binary_matrix.copy()\n",
    "    return kw_matrix\n",
    "def wik_sim_calculate(preprocessed_inputs, wik_dict):\n",
    "    wik_sim={}\n",
    "    for i in range(len(preprocessed_inputs)):\n",
    "        for j in range(len(preprocessed_inputs)):\n",
    "            if i!=j:\n",
    "                wik_sim[(i,j)]=cos_sim(list(wik_dict[i].values()),list(wik_dict[j].values()))\n",
    "            else:\n",
    "                wik_sim[(i,j)]=0\n",
    "\n",
    "    return wik_sim\n",
    "\n",
    "def o_wik_sim_calculate(preprocessed_inputs,o_dict,wik_dict):\n",
    "    o_wik_sim={}\n",
    "    for i in range(len(preprocessed_inputs)):\n",
    "        o_wik_sim[i]=cos_sim(list(wik_dict[i].values()),list(o_dict.values()))\n",
    "    return o_wik_sim\n",
    "\n",
    "\n",
    "class kw_topic_information_class:\n",
    "    def __init__(self,preprocessed_inputs,topic_to_words_mapper,topic_assignment_matrix,keywords,wik_dict,o_dict):\n",
    "        self.preprocessed_inputs=preprocessed_inputs\n",
    "        self.topic_to_words_mapper=topic_to_words_mapper\n",
    "        self.topic_assignment_matrix=topic_assignment_matrix\n",
    "        self.keywords=keywords\n",
    "        self.kw_multipliers_dict=self.kw_multipliers_init(self.preprocessed_inputs,self.keywords)\n",
    "        self.topic_multipliers_dict=self.topic_multipliers_init(self.preprocessed_inputs,self.topic_to_words_mapper,self.topic_assignment_matrix)\n",
    "        self.wik_sim=self.wik_sim_calculate(preprocessed_inputs,wik_dict)\n",
    "        self.o_wik_sim=self.o_wik_sim_calculate(preprocessed_inputs,o_dict,wik_dict)\n",
    "        \n",
    "\n",
    "        \n",
    "    def wik_sim_calculate(self, preprocessed_inputs, wik_dict):\n",
    "        wik_sim={}\n",
    "        for i in range(len(preprocessed_inputs)):\n",
    "            for j in range(len(preprocessed_inputs)):\n",
    "                if i!=j:\n",
    "                    wik_sim[(i,j)]=cos_sim(list(wik_dict[i].values()),list(wik_dict[j].values()))\n",
    "                else:\n",
    "                    wik_sim[(i,j)]=0\n",
    "                    \n",
    "        return wik_sim\n",
    "   \n",
    "    def o_wik_sim_calculate(self,preprocessed_inputs,o_dict,wik_dict):\n",
    "        o_wik_sim={}\n",
    "        for i in range(len(preprocessed_inputs)):\n",
    "            o_wik_sim[i]=cos_sim(list(wik_dict[i].values()),list(o_dict.values()))\n",
    "        return o_wik_sim\n",
    "    \n",
    "    \n",
    "    def topic_multipliers_init(self,preprocessed_inputs,topic_to_words_mapper,topic_assignment_matrix):\n",
    "        topic_multipliers_dict={}\n",
    "        for i in range(len(preprocessed_inputs)):\n",
    "            for j in range(len(preprocessed_inputs)):\n",
    "                if i!=j:\n",
    "                    terms_i=preprocessed_inputs[i]\n",
    "                    terms_i_set=set(terms_i)\n",
    "                    terms_j=preprocessed_inputs[j]\n",
    "                    terms_j_set=set(terms_j)\n",
    "        \n",
    "                    topic_i=topic_to_words_mapper[np.argmax(np.array(topic_assignment_matrix[i]))]\n",
    "                    topic_i_set=set(topic_i)\n",
    "                    topic_j=topic_to_words_mapper[np.argmax(np.array(topic_assignment_matrix[j]))]\n",
    "                    topic_j_set=set(topic_j)\n",
    "                    denom=len(topic_i_set.union(topic_j_set)) \n",
    "\n",
    "                    num=len(terms_i_set.intersection(terms_j_set,topic_i_set,topic_j_set))\n",
    "                    topic_multipliers_dict[(i,j)]=((num+1)/(denom+1))\n",
    "        return topic_multipliers_dict\n",
    "    def kw_multipliers_init(self,preprocessed_inputs,keywords):\n",
    "        kw_multipliers_dict={}\n",
    "        for i in range(len(preprocessed_inputs)):\n",
    "            for j in range(len(preprocessed_inputs)):\n",
    "                if i!=j:\n",
    "                    vec_i=keywords[i]\n",
    "                    vec_j=keywords[j]\n",
    "                    num_matches = np.sum(np.logical_and(vec_i == 1, vec_j == 1))\n",
    "                    num_mismatches = np.sum(np.logical_xor(vec_i == 1, vec_j == 1))\n",
    "                    kw_multipliers_dict[(i,j)]=((num_matches+1)/(num_mismatches+1))\n",
    "        return kw_multipliers_dict\n",
    "\n",
    "def rouge_score_calc(extract,list_of_sentences,all_rouge_scores):\n",
    "    rouge_score_dict={}\n",
    "    rouge_score_dict[\"consensus\"]={\"candidate\":all_rouge_scores.consensus_extract.mem,\"objective\":all_rouge_scores.consensus_extract.objective}\n",
    "    \n",
    "    rouge_score_dict[\"largest_hypervolume_extract\"]={\"candidate\":all_rouge_scores.largest_hypervolume_extract.mem,\"objective\":all_rouge_scores.largest_hypervolume_extract.objective}\n",
    "    rouge_score_dict[\"topmost_coverage_extract\"]={\"candidate\":all_rouge_scores.topmost_coverage_extract.mem,\"objective\":all_rouge_scores.topmost_coverage_extract.objective}\n",
    "    rouge_score_dict[\"topmost_redundancy_reduction\"]={\"candidate\":all_rouge_scores.topmost_redundancy_reduction_extract.mem,\"objective\":all_rouge_scores.topmost_redundancy_reduction_extract.objective}\n",
    "    rouge_score_dict[\"shortest_point_to_ideal_point_euclidean\"]={\"candidate\":all_rouge_scores.shortest_point_to_ideal_point_euclidean_extract.mem,\"objective\":all_rouge_scores.shortest_point_to_ideal_point_euclidean_extract.objective}\n",
    "    rouge_score_dict[\"shortest_point_to_ideal_point_manhattan\"]={\"candidate\":all_rouge_scores.shortest_point_to_ideal_point_manhattan_extract.mem,\"objective\":all_rouge_scores.shortest_point_to_ideal_point_manhattan_extract.objective}\n",
    "    rouge_score_dict[\"shortest_point_to_ideal_point_chebyshev\"]={\"candidate\":all_rouge_scores.shortest_point_to_ideal_point_chebyshev_extract.mem,\"objective\":all_rouge_scores.shortest_point_to_ideal_point_chebyshev_extract.objective}\n",
    "    \n",
    "    rouge_score_dict[\"shortest_point_to_ideal_point_mahalanobis\"]={\"candidate\":all_rouge_scores.shortest_point_to_ideal_point_mahalanobis_extract.mem,\"objective\":all_rouge_scores.shortest_point_to_ideal_point_mahalanobis_extract.objective}\n",
    "    rouge_score_dict[\"shortest_point_to_all_points_chebyshev\"]={\"candidate\":all_rouge_scores.shortest_point_to_all_points_chebyshev_extract.mem,\"objective\":all_rouge_scores.shortest_point_to_all_points_chebyshev_extract.objective}\n",
    "    \n",
    "    rouge_score_dict[\"shortest_point_to_all_points_manhattan\"]={\"candidate\":all_rouge_scores.shortest_point_to_all_points_manhattan_extract.mem,\"objective\":all_rouge_scores.shortest_point_to_all_points_manhattan_extract.objective}\n",
    "    rouge_score_dict[\"shortest_point_to_all_points_mahalanobis\"]={\"candidate\":all_rouge_scores.shortest_point_to_all_points_mahalanobis_extract.mem,\"objective\":all_rouge_scores.shortest_point_to_all_points_mahalanobis_extract.objective}\n",
    "    rouge_score_dict[\"shortest_point_to_all_points_euclidean\"]={\"candidate\":all_rouge_scores.shortest_point_to_all_points_euclidean_extract.mem,\"objective\":all_rouge_scores.shortest_point_to_all_points_euclidean_extract.objective}\n",
    "    rouge_score_dict[\"shortest_point_to_all_points_levenshtein\"]={\"candidate\":all_rouge_scores.shortest_point_to_all_points_levenshtein_extract.mem,\"objective\":all_rouge_scores.shortest_point_to_all_points_levenshtein_extract.objective}\n",
    "\n",
    "    for k,v in rouge_score_dict.items():\n",
    "        predicted_extract=\"\"\n",
    "        candidate=v[\"candidate\"]\n",
    "        for i in range(len(candidate)):\n",
    "            if candidate[i]==1:\n",
    "                predicted_extract+= list_of_sentences[i] + \" . \"\n",
    "                \n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(extract, predicted_extract)\n",
    "        \n",
    "        v[\"predicted_extract\"]=predicted_extract\n",
    "        r1_d={\"precision\":scores[\"rouge1\"][0],\"recall\":scores[\"rouge1\"][1],\"fmeasure\":scores[\"rouge1\"][2]}\n",
    "        r2_d={\"precision\":scores[\"rouge2\"][0],\"recall\":scores[\"rouge2\"][1],\"fmeasure\":scores[\"rouge2\"][2]}\n",
    "        rl_d={\"precision\":scores[\"rougeL\"][0],\"recall\":scores[\"rougeL\"][1],\"fmeasure\":scores[\"rougeL\"][2]}\n",
    "        v[\"rouge_score_detail\"]={\"rouge1\":r1_d,\"rouge2\":r2_d,\"rougeL\":rl_d}\n",
    "        \n",
    "        print_pattern()\n",
    "        \n",
    "        print(\"REFERENCE EXTRACT : \")\n",
    "        print(extract)\n",
    "        print(\"PREDICTED EXTRACT : \")\n",
    "        print(predicted_extract)\n",
    "        print(\"ROUGE SCORE       : \")\n",
    "        print(v['rouge_score_detail'])\n",
    "        \n",
    "        print_pattern()\n",
    "\n",
    "\n",
    "    return rouge_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.135668Z",
     "iopub.status.idle": "2024-03-17T14:37:07.136175Z",
     "shell.execute_reply": "2024-03-17T14:37:07.135923Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.135902Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_single_result(NDS_archive,doc_info):\n",
    "    list_of_sentences=doc_info.list_of_sentences\n",
    "    L=doc_info.L\n",
    "    epsilon=doc_info.epsilon\n",
    "    all_rouge_scores=rouge_score_details(NDS_archive,doc_info)\n",
    "    return all_rouge_scores\n",
    "\n",
    "def add_result_line(file_path, text):\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(text + '\\n')\n",
    "\n",
    "\n",
    "def return_current_datetime_string():\n",
    "    current_datetime = datetime.now()\n",
    "    current_date_formatted = current_datetime.strftime('%d-%m-%y')\n",
    "    current_time = datetime.now().time()\n",
    "    hours_minutes = current_time.strftime('%H-%M')\n",
    "    date_time_string=current_date_formatted+\"_\"+hours_minutes\n",
    "    return date_time_string\n",
    "\n",
    "def repair_all(bees_list,doc_info):\n",
    "    new_bees_list=[]\n",
    "    \n",
    "    for bee in bees_list:\n",
    "        time1=timeit.default_timer()\n",
    "        repaired_mem=fast_repair_operator(bee.mem,doc_info)[0]\n",
    "        new_bees_list.append(member(repaired_mem,doc_info))\n",
    "        time2=timeit.default_timer()\n",
    "#         print(\"Repair for this bee : \",time2-time1)\n",
    "    return new_bees_list.copy()\n",
    "def textrank_scores_calculate(wik_dict,list_of_sentences):\n",
    "    similarity_matrix = np.zeros([len(list_of_sentences), len(list_of_sentences)])\n",
    "    for i,row in wik_dict.items():\n",
    "        for j,column in wik_dict.items():\n",
    "            row_embedding=list(row.values())\n",
    "            column_embedding=list(column.values())\n",
    "            similarity_matrix[i][j]=cos_sim(row_embedding,column_embedding)\n",
    "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    textrank_scores=dict(sorted(scores.items(), key=lambda x:x[1],reverse=True))\n",
    "    return textrank_scores\n",
    "    \n",
    "def sentence_scores_calculation(tfik_dict,term_to_index_mapper,index_to_term_mapper):\n",
    "    total_term_occ_dict={}\n",
    "    for i in index_to_term_mapper.keys():\n",
    "        total_term_occ_dict[i]=0\n",
    "        \n",
    "    for k,v in tfik_dict.items():\n",
    "        for word,word_count in v.items():\n",
    "            total_term_occ_dict[term_to_index_mapper[word]] += word_count\n",
    "    \n",
    "    \n",
    "    scores_dict={}\n",
    "    for i in tfik_dict.keys():\n",
    "        scores_dict[i]=0\n",
    "    for sentence_index,v in tfik_dict.items():\n",
    "        for word,word_count in v.items():\n",
    "            scores_dict[sentence_index]+=total_term_occ_dict[term_to_index_mapper[word]]\n",
    "    \n",
    "    for sentence_index,v in scores_dict.items():\n",
    "        scores_dict[sentence_index]/=10\n",
    "    return scores_dict\n",
    "def lexrank_model(dataset_access_path):\n",
    "\n",
    "    documents = []\n",
    "    documents_dir = Path(dataset_access_path)\n",
    "    for file_path in documents_dir.files('*full_text.txt'):\n",
    "        with file_path.open(mode='rt', encoding='utf-8') as fp:\n",
    "            documents.append(fp.readlines())\n",
    "            \n",
    "    lxr = LexRank(documents, stopwords=STOPWORDS['en'])\n",
    "    \n",
    "    return lxr\n",
    "\n",
    "\n",
    "class scores_class:\n",
    "    def __init__(self,tf_scores,textrank_scores,graph_scores,lexrank_scores):\n",
    "        self.tf_scores=tf_scores\n",
    "        self.textrank_scores=textrank_scores\n",
    "        self.graph_scores=graph_scores\n",
    "        self.lexrank_scores=lexrank_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.137729Z",
     "iopub.status.idle": "2024-03-17T14:37:07.138417Z",
     "shell.execute_reply": "2024-03-17T14:37:07.138176Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.138155Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class tracker:\n",
    "    def __init__(self,population_members,time_taken,NDS_archive_ct,NDS_archive_growth,archive_tracker):\n",
    "        self.population_members=population_members\n",
    "        self.time_taken=time_taken\n",
    "        self.NDS_archive_ct=NDS_archive_ct\n",
    "        self.NDS_archive_growth=NDS_archive_growth\n",
    "        self.archive_tracker=archive_tracker\n",
    "def run_MOABC(doc_info):\n",
    "    NDS_archive = []\n",
    "    archive_tracker = {}\n",
    "    all_sols = []\n",
    "    cycles=doc_info.cycles\n",
    "    list_of_members = initialize_population(doc_info)\n",
    "    population_tracker=[]\n",
    "    for cycle in range(cycles):\n",
    "        archive_format = {\"employed\": [], \"onlooker\": [], \"scoutbeforerepair\": [], \"scoutafterrepair\": []}\n",
    "        \n",
    "        prev_ct = len(NDS_archive)\n",
    "        start = timeit.default_timer()\n",
    "        \n",
    "        mutated_member_list = send_employed_bees(list_of_members.copy(), doc_info)\n",
    "        NDS_archive = NDS_insert_ver_6(NDS_archive, mutated_member_list, doc_info).copy()\n",
    "        archive_format[\"employed\"] = NDS_archive.copy()\n",
    "\n",
    "        onlooker_bee_list = send_onlooker_bees(mutated_member_list, doc_info)\n",
    "        NDS_archive = NDS_insert_ver_6(NDS_archive, onlooker_bee_list, doc_info).copy()\n",
    "        archive_format[\"onlooker\"] = NDS_archive.copy()\n",
    "        \n",
    "        ranked_list_of_members = rank_and_crowd_onlookers(onlooker_bee_list, doc_info)\n",
    "        scout_bees_list = send_scout_bees(ranked_list_of_members, doc_info)\n",
    "        NDS_archive = NDS_insert_ver_6(NDS_archive, scout_bees_list, doc_info).copy()\n",
    "        archive_format[\"scoutbeforerepair\"] = NDS_archive.copy()\n",
    "        \n",
    "        scout_bees_list = repair_all(scout_bees_list.copy(), doc_info).copy()\n",
    "        NDS_archive = NDS_insert_ver_6(NDS_archive, scout_bees_list, doc_info).copy()\n",
    "        archive_format[\"scoutafterrepair\"] = NDS_archive.copy()\n",
    "        list_of_members = scout_bees_list.copy()\n",
    "        \n",
    "        end = timeit.default_timer()\n",
    "        new_ct = len(NDS_archive)\n",
    "        print(\"Cycle = \", cycle, \"Completed, Len of Archive =\", len(NDS_archive), \",  No of solns added/removed = \", new_ct - prev_ct, \"Time taken : \",round(end-start,2))\n",
    "\n",
    "        archive_tracker[cycle] = archive_format\n",
    "        doc_info.NDS_archive=NDS_archive\n",
    "        population_tracker.append(tracker(list_of_members,round(end-start,2),new_ct,new_ct-prev_ct,archive_format))\n",
    "\n",
    "    return NDS_archive.copy(), archive_tracker,population_tracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.139760Z",
     "iopub.status.idle": "2024-03-17T14:37:07.140269Z",
     "shell.execute_reply": "2024-03-17T14:37:07.140031Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.140011Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def NSGA_replace(list_of_members,current_mem,new_mem,i):\n",
    "    result=dominate(current_mem.fitness,new_mem.fitness)\n",
    "    if result==0:\n",
    "        print(\"Not\",current_mem.fitness,new_mem.fitness)\n",
    "    elif result==1:\n",
    "        print(\"WON\",current_mem.fitness,new_mem.fitness)\n",
    "    elif result==\"False\":\n",
    "        #print(\"Non Dominating each other\",current_mem.fitness,new_mem.fitness)\n",
    "        pass\n",
    "    \n",
    "    list_of_members[i]=new_mem\n",
    "    return list_of_members\n",
    "\n",
    "\n",
    "def NSGA_mutation(list_of_members, doc_info):\n",
    "    start_time = time.time()  # Start timing the function\n",
    "    no_of_members = len(list_of_members)\n",
    "    l = []\n",
    "    for i in range(no_of_members):\n",
    "        current_mem = list_of_members[i]\n",
    "#         time_before_mutation = time.time()\n",
    "        new_mem_list = mutation_operator_select(current_mem.mem.copy(), doc_info)\n",
    "#         time_after_mutation = time.time()\n",
    "        new_mem = member(new_mem_list.copy(), doc_info)\n",
    "#         time_after_member_creation = time.time()\n",
    "        \n",
    "        \n",
    "        list_of_members=replacement_operator_select(list_of_members.copy(),current_mem,new_mem,i,doc_info)\n",
    "#         time_after_replace = time.time()\n",
    "        \n",
    "#         print(\"Time for mutation:\", time_after_mutation - time_before_mutation)\n",
    "#         print(\"Time for member creation:\", time_after_member_creation - time_after_mutation)\n",
    "#         print(\"Time for replace\", time_after_replace - time_after_member_creation)\n",
    "\n",
    "    return list_of_members\n",
    "\n",
    "def BCTSO(parent_members,child_members,doc_info):\n",
    "    list_of_members=parent_members.copy()\n",
    "    list_of_members.extend(child_members)\n",
    "    no_of_objectives=doc_info.no_of_objectives\n",
    "    front_info_list=list(front_info_former(list_of_members,doc_info).values())\n",
    "    \n",
    "    child_population=[]\n",
    "    for i in range(len(parent_members)):\n",
    "        tournament_pair=random.choice(list(combinations(range(len(list_of_members)), 2)))\n",
    "        mi=tournament_pair[0]\n",
    "        mj=tournament_pair[1]\n",
    "        if front_info_list[mi].rank !=front_info_list[mj].rank:\n",
    "            if front_info_list[mi].rank<front_info_list[mj].rank:\n",
    "                #print(list_of_members[mi].objective,front_info_list[mi].rank, list_of_members[mj].objective,front_info_list[mj].rank)\n",
    "                child_population.append(list_of_members[mi])\n",
    "            else:\n",
    "                #print(list_of_members[mi].objective,front_info_list[mi].rank, list_of_members[mj].objective,front_info_list[mj].rank)\n",
    "                child_population.append(list_of_members[mj])\n",
    "        elif front_info_list[mi].rank==front_info_list[mj].rank:\n",
    "            if front_info_list[mi].cd==front_info_list[mj].cd:\n",
    "                child_population.append(random.choice([list_of_members[mi],list_of_members[mj]]))\n",
    "            elif front_info_list[mi].cd<front_info_list[mj].cd:\n",
    "                child_population.append(list_of_members[mj])\n",
    "            else:\n",
    "                child_population.append(list_of_members[mi])\n",
    "        \n",
    "    return child_population.copy()\n",
    "    \n",
    "def run_NSGA(doc_info):\n",
    "    NDS_archive = []\n",
    "    archive_tracker = {}\n",
    "    all_sols = []\n",
    "    cycles=doc_info.cycles\n",
    "    list_of_members = initialize_population(doc_info)\n",
    "    population_tracker=[]\n",
    "    for cycle in range(cycles):\n",
    "        \n",
    "        prev_ct = len(NDS_archive)\n",
    "        archive_format = {\"final\": []}\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        \n",
    "        list_of_members=repair_all(list_of_members.copy(),doc_info).copy()\n",
    "        LOF=list_of_members.copy()\n",
    "        mutated_member_list = NSGA_mutation(LOF, doc_info)\n",
    "        mutated_member_list = repair_all(mutated_member_list.copy(), doc_info)\n",
    "        NDS_archive = NDS_insert_ver_6(NDS_archive, list_of_members, doc_info)\n",
    "        NDS_archive = NDS_insert_ver_6(NDS_archive, mutated_member_list, doc_info)\n",
    "        \n",
    "        list_of_members=BCTSO(list_of_members.copy(),mutated_member_list.copy(),doc_info)\n",
    "#         NDS_archive = NDS_insert_ver_6(NDS_archive, list_of_members, doc_info).copy()\n",
    "        archive_format[\"final\"]=NDS_archive.copy()\n",
    "\n",
    "        new_ct = len(NDS_archive)\n",
    "        end=timeit.default_timer()\n",
    "        print(\"Cycle = \", cycle, \"Completed, Len of Archive =\", len(NDS_archive), \",  No of solns added/removed = \", new_ct - prev_ct, \"Time taken : \",round(end-start,2))\n",
    "        archive_tracker[cycle] = archive_format\n",
    "        doc_info.NDS_archive=NDS_archive\n",
    "        \n",
    "        population_tracker.append(tracker(list_of_members,round(end-start,2),new_ct,new_ct-prev_ct,archive_format))\n",
    "\n",
    "    return NDS_archive.copy(), archive_tracker,population_tracker\n",
    "\n",
    "def transform_tvectors(sentence_probabilities):\n",
    "    wik_dict={}\n",
    "    o_dict={}\n",
    "    o_vec=np.mean(sentence_probabilities.T,axis=1)\n",
    "    for i in range(len(sentence_probabilities)):\n",
    "        wik_dict[i]={}\n",
    "        for k in range(len(sentence_probabilities[i])):\n",
    "            wik_dict[i][k]=sentence_probabilities[i][k]\n",
    "    for i in range(len(sentence_probabilities[0])):\n",
    "        o_dict[i]=o_vec[i]\n",
    "    return wik_dict,o_dict\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.142115Z",
     "iopub.status.idle": "2024-03-17T14:37:07.142585Z",
     "shell.execute_reply": "2024-03-17T14:37:07.142361Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.142341Z"
    }
   },
   "outputs": [],
   "source": [
    "def rcd_based_replacement(list_of_members,current_mem,new_mem,i,doc_info):\n",
    "    temp_list=list_of_members.copy()\n",
    "    temp_list.extend([new_mem])\n",
    "    no_of_objectives=doc_info.no_of_objectives\n",
    "    front_info_list=list(front_info_former(temp_list,doc_info).values())\n",
    "\n",
    "    mi=i\n",
    "    mj=len(temp_list)-1\n",
    "    if front_info_list[mi].rank !=front_info_list[mj].rank:\n",
    "        if front_info_list[mi].rank<front_info_list[mj].rank:\n",
    "            list_of_members[i].trials+=1\n",
    "            pass\n",
    "        else:\n",
    "            list_of_members=naive_replace(list_of_members,current_mem,new_mem,i)\n",
    "    elif front_info_list[mi].rank==front_info_list[mj].rank:\n",
    "        if front_info_list[mi].cd==front_info_list[mj].cd:\n",
    "            list_of_members[i].trials+=1\n",
    "            pass\n",
    "        elif front_info_list[mi].cd<front_info_list[mj].cd:\n",
    "            list_of_members=naive_replace(list_of_members,current_mem,new_mem,i)\n",
    "        else:\n",
    "            list_of_members[i].trials+=1\n",
    "            pass\n",
    "        \n",
    "    return list_of_members\n",
    "\n",
    "def dominate_based_replacement(list_of_members,current_mem,new_mem,i,doc_info):\n",
    "    result=dominate(current_mem.fitness,new_mem.fitness)\n",
    "    if result==0:\n",
    "        list_of_members[i].trials+=1\n",
    "    elif result==1:\n",
    "        list_of_members=naive_replace(list_of_members,current_mem,new_mem,i)\n",
    "    elif result==\"False\":\n",
    "        list_of_members[i].trials+=1\n",
    "    return list_of_members\n",
    "\n",
    "def epsilon_dominate_based_replacement(list_of_members,current_mem,new_mem,i,doc_info):\n",
    "    result=dominate(current_mem.fitness,new_mem.fitness)\n",
    "    if result==1:\n",
    "        list_of_members=naive_replace(list_of_members,current_mem,new_mem,i)\n",
    "    else: \n",
    "        #in cases of Nondominating or 0 conditions\n",
    "        percents={}\n",
    "        validity={}\n",
    "        percent_imp=1\n",
    "        percent_dec=-0.25\n",
    "        winners=0\n",
    "        no_of_objectives=doc_info.no_of_objectives\n",
    "        for index in range(no_of_objectives):\n",
    "            percents[index]=(new_mem.fitness[index]-current_mem.fitness[index])/current_mem.fitness[index]\n",
    "            if new_mem.fitness[index] >= current_mem.fitness[index]:\n",
    "                # %improvement case\n",
    "                if percents[index] >= percent_imp:\n",
    "                    validity[index]=True\n",
    "                    winners+=1\n",
    "                else:\n",
    "                    validity[index]=False\n",
    "                \n",
    "            else :\n",
    "                if percents[index] >= percent_dec:\n",
    "                    validity[index]=True\n",
    "                else:\n",
    "                    validity[index]=False\n",
    "        if list(validity.values()) == [True] *no_of_objectives and winners>= int(no_of_objectives/2):\n",
    "            list_of_members=naive_replace(list_of_members,current_mem,new_mem,i)\n",
    "        else:\n",
    "            list_of_members[i].trials+=1\n",
    "    \n",
    "    return list_of_members\n",
    "\n",
    "def replacement_operator_select(list_of_members,current_mem,new_mem,i,doc_info):\n",
    "    replacement_operator_version=doc_info.replacement_operator_version\n",
    "#     if doc_info.model_version==\n",
    "    if replacement_operator_version==\"rcd\":\n",
    "        return rcd_based_replacement(list_of_members,current_mem,new_mem,i,doc_info)\n",
    "    elif replacement_operator_version==\"dominate\":\n",
    "        return dominate_based_replacement(list_of_members,current_mem,new_mem,i,doc_info)\n",
    "    elif replacement_operator_version==\"epsilon_dominate\":\n",
    "        return epsilon_dominate_based_replacement(list_of_members,current_mem,new_mem,i,doc_info)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.144134Z",
     "iopub.status.idle": "2024-03-17T14:37:07.144455Z",
     "shell.execute_reply": "2024-03-17T14:37:07.144312Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.144298Z"
    }
   },
   "outputs": [],
   "source": [
    "class saver_class:\n",
    "    #full_text_file_path,greedy_extract_file_path,original_text_content,L,revised_text_file_content,preprocessed_inputs,preprocessed_inputs_no_checks,list_of_sentences,index_to_sentence_info_mapper,tfik_dict,no_of_sentences,terms_dict,no_of_terms,index_to_term_mapper,term_to_index_mapper,wik_dict,o_dict,kw_topic_info,\n",
    "    def __init__(self,doc_info,NDS_archive,greedy_extract_text,rouge_score,NDS_archive_tracker,params,population_tracker):\n",
    "        self.doc_info=doc_info\n",
    "        self.NDS_archive=NDS_archive\n",
    "        self.greedy_extract_text=greedy_extract_text\n",
    "        self.rouge_score=rouge_score\n",
    "        self.NDS_archive_tracker=NDS_archive_tracker\n",
    "        self.params=params\n",
    "        self.population_tracker=population_tracker\n",
    "\n",
    "def save_details(saver_class,path,version,datetime_string,article_id):\n",
    "    with open(path+version+\"_\"+article_id+\".pickle\",\"wb\") as file:\n",
    "        pickle.dump(saver_class,file)\n",
    "\n",
    "\n",
    "class file_paths:\n",
    "    def __init__(self,base_path_input,base_path_output,version,current_datetime_string,kaggle):\n",
    "        if kaggle==True:\n",
    "            self.base_path_input=base_path_input\n",
    "            self.dataset_path=self.base_path_input +\"/datasets/\"\n",
    "            self.dataset_access_path=self.base_path_input +\"/datasets/individual_1/\"\n",
    "\n",
    "\n",
    "            self.base_path_output=base_path_output\n",
    "            self.result_path=self.base_path_output +\"/results/\"\n",
    "            self.details_path=self.base_path_output+\"/details/\"\n",
    "\n",
    "            self.version=version\n",
    "            self.current_datetime_string=current_datetime_string\n",
    "            self.result_file_name=self.version+\"_\"+current_datetime_string +\".txt\"\n",
    "        else:\n",
    "            self.base_path=base_path_input\n",
    "            self.result_path=self.base_path +\"\\\\results\\\\\"\n",
    "            self.dataset_path=self.base_path +\"\\\\datasets\\\\\"\n",
    "            self.dataset_access_path=self.base_path +\"\\\\datasets\\\\individual_1\\\\\"\n",
    "            self.details_path=self.base_path+\"\\\\details\\\\\"\n",
    "            self.version=version\n",
    "            self.current_datetime_string=current_datetime_string\n",
    "            self.result_file_name=self.version+\"_\"+current_datetime_string +\".txt\"\n",
    "\n",
    "class compressed_doc_info_class:\n",
    "    def __init__(self,preprocessed_inputs,list_of_sentences,L,epsilon):\n",
    "        self.preprocessed_inputs=preprocessed_inputs\n",
    "        self.list_of_sentences=list_of_sentences\n",
    "        self.L=L\n",
    "        self.epsilon=epsilon\n",
    "class doc_info_class:\n",
    "    def __init__(self, full_text_file_path,greedy_extract_file_path,original_text_content,L,revised_text_file_content,preprocessed_inputs,list_of_sentences,article_id,tfik_dict,no_of_sentences,terms_dict,no_of_terms,index_to_term_mapper,term_to_index_mapper,wik_dict,o_dict,kw_topic_info,scores_details,sentence_lengths,params,sentence_probabilities,file_paths_class,HDP_topics):\n",
    "        self.full_text_file_path=full_text_file_path\n",
    "        self.greedy_extract_file_path=greedy_extract_file_path\n",
    "        self.original_text_content=original_text_content\n",
    "        self.L=L\n",
    "        self.revised_text_file_content=revised_text_file_content\n",
    "        self.preprocessed_inputs=preprocessed_inputs\n",
    "#         self.preprocessed_inputs_no_checks=preprocessed_inputs_no_checks\n",
    "        self.list_of_sentences=list_of_sentences\n",
    "        #self.index_to_sentence_info_mapper=index_to_sentence_info_mapper\n",
    "        self.article_id=article_id\n",
    "        self.tfik_dict=tfik_dict\n",
    "        self.no_of_sentences=no_of_sentences\n",
    "        self.terms_dict=terms_dict\n",
    "        self.no_of_terms=no_of_terms\n",
    "        self.index_to_term_mapper=index_to_term_mapper\n",
    "        self.term_to_index_mapper=term_to_index_mapper\n",
    "        self.wik_dict=wik_dict\n",
    "        self.o_dict=o_dict\n",
    "        self.kw_topic_info=kw_topic_info\n",
    "        self.o_wik_sim=self.kw_topic_info.o_wik_sim\n",
    "        self.wik_sim=self.kw_topic_info.wik_sim\n",
    "        self.scores_details=scores_details\n",
    "        self.no_of_objectives=params.no_of_objectives\n",
    "        self.doc_threshold_value=params.doc_threshold_value\n",
    "        self.sentence_threshold_value=params.sentence_threshold_value\n",
    "        self.spacy_use=params.spacy_use\n",
    "        self.restricted_docs_list=params.restricted_docs_list\n",
    "        self.epsilon=params.epsilon\n",
    "        self.L=params.L\n",
    "        self.no_of_objectives=params.no_of_objectives\n",
    "        self.pop_size=params.pop_size\n",
    "        self.pm=params.pm\n",
    "        self.trial_cutoff_limit=params.trial_cutoff_limit\n",
    "        self.cycles=params.cycles\n",
    "        self.sentence_lengths=sentence_lengths\n",
    "        self.delta=params.delta\n",
    "        self.NDS_archive=[]\n",
    "        self.t_wik_dict,self.t_o_dict=transform_tvectors(sentence_probabilities)\n",
    "        self.t_wik_sim=wik_sim_calculate(self.preprocessed_inputs,self.t_wik_dict)\n",
    "        self.t_o_wik_sim=o_wik_sim_calculate(self.preprocessed_inputs,self.t_o_dict,self.t_wik_dict)\n",
    "        self.mutation_operator_version=params.mutation_operator_version\n",
    "        self.replacement_operator_version=params.replacement_operator_version\n",
    "        self.algorithm=params.algorithm\n",
    "        self.IP_population_initialization=params.IP_population_initialization\n",
    "        self.topic_enabled=params.topic_enabled\n",
    "        self.sbert_enabled=params.sbert_enabled\n",
    "        self.kaggle=params.kaggle\n",
    "        self.run=params.run\n",
    "        self.kaggle=params.kaggle\n",
    "        self.delim=params.delim\n",
    "        \n",
    "        self.result_path=file_paths_class.result_path\n",
    "        self.dataset_path=file_paths_class.dataset_path\n",
    "        self.result_file_name=file_paths_class.result_file_name\n",
    "        self.version=file_paths_class.version\n",
    "        self.details_path=file_paths_class.details_path\n",
    "        self.dataset_access_path=file_paths_class.dataset_access_path\n",
    "        \n",
    "        self.HDP_topics=HDP_topics\n",
    "        \n",
    "        self.csv_name=params.csv_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.145867Z",
     "iopub.status.idle": "2024-03-17T14:37:07.146226Z",
     "shell.execute_reply": "2024-03-17T14:37:07.146058Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.146043Z"
    }
   },
   "outputs": [],
   "source": [
    "class GETS:\n",
    "    def __init__(self,doc_info):\n",
    "        self.doc_info=doc_info\n",
    "        self.edge_weights=self.edge_weights_computation(doc_info)\n",
    "        self.sentence_weights=self.sentence_weights_computation(doc_info,self.edge_weights)\n",
    "        self.average_weight=self.average_weight_computation(doc_info,self.sentence_weights)\n",
    "        self.summary,self.summary_length=self.gets_summarizer(doc_info,self.sentence_weights,self.average_weight)\n",
    "        self.gets_sentence_scores=self.return_GETS_scores(self.sentence_weights)\n",
    "    def edge_weights_computation(self,doc_info):\n",
    "        edge_weights={}\n",
    "        preprocessed_inputs=doc_info.preprocessed_inputs\n",
    "        for i in range(len(preprocessed_inputs)):\n",
    "            for j in range(len(preprocessed_inputs)):\n",
    "                if i!=j :\n",
    "                    edge_weights[(i,j)]=len(set(preprocessed_inputs[i]).intersection(preprocessed_inputs[j]))\n",
    "                else:\n",
    "                    edge_weights[(i,j)]=0\n",
    "        \n",
    "        return edge_weights\n",
    "    \n",
    "    def sentence_weights_computation(self,doc_info,edge_weights):\n",
    "        sentence_weights={}\n",
    "        preprocessed_inputs=doc_info.preprocessed_inputs\n",
    "        for i in range(len(preprocessed_inputs)):\n",
    "            sentence_weights[i]=0\n",
    "        \n",
    "        for i in range(len(preprocessed_inputs)):\n",
    "            for j in range(i+1,len(preprocessed_inputs)):\n",
    "                sentence_weights[i]+=edge_weights[(i,j)]\n",
    "        return sentence_weights\n",
    "    \n",
    "    def average_weight_computation(self,doc_info,sentence_weights):\n",
    "        preprocessed_inputs=doc_info.preprocessed_inputs\n",
    "        weighted_sum=0\n",
    "        for i in range(len(preprocessed_inputs)):\n",
    "            weighted_sum+=sentence_weights[i]\n",
    "        \n",
    "        weighted_sum/=len(preprocessed_inputs)\n",
    "        weighted_sum=math.ceil(weighted_sum)\n",
    "        \n",
    "        return weighted_sum\n",
    "    \n",
    "    def gets_summarizer(self,doc_info,sentence_weights,average_weight):\n",
    "        Lmax=doc_info.L+doc_info.epsilon\n",
    "        Lmin=doc_info.L-doc_info.epsilon\n",
    "        new_revised_sentences={}\n",
    "        for k,v in sentence_weights.items():\n",
    "            if v >= average_weight:\n",
    "                new_revised_sentences[k]=v\n",
    "        sorted_sentences= dict(sorted(new_revised_sentences.items(), key=lambda item: item[1],reverse=True))\n",
    "        \n",
    "        cur_summary=\"\"\n",
    "        cur_summary_length=0\n",
    "        list_of_sentences=doc_info.list_of_sentences\n",
    "        for k,v in sorted_sentences.items():\n",
    "            if cur_summary_length+len(list_of_sentences[k].split()) >Lmax:\n",
    "                break\n",
    "            else:\n",
    "                cur_summary_length+=len(list_of_sentences[k].split())\n",
    "                cur_summary+=list_of_sentences[k]+\" . \"\n",
    "        \n",
    "        return cur_summary,cur_summary_length\n",
    "    \n",
    "    def return_GETS_scores(self,sentence_weights):\n",
    "        sorted_sentence_weights= dict(sorted(sentence_weights.items(), key=lambda item: item[1],reverse=True))\n",
    "        return sorted_sentence_weights\n",
    "        \n",
    "            \n",
    "\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.147482Z",
     "iopub.status.idle": "2024-03-17T14:37:07.147858Z",
     "shell.execute_reply": "2024-03-17T14:37:07.147699Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.147684Z"
    }
   },
   "outputs": [],
   "source": [
    "special_characters = ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '_', '+', '-', '=', '{', '}', '[', ']', '|', '\\\\', ':', ';', '<', '>', ',', '/', '?', '\"' ,\"`\",\"``\",\"'\" ]\n",
    "\n",
    "def clean_text(input_string):\n",
    "    translation_table = str.maketrans('', '', ''.join(special_characters))\n",
    "    cleaned_string = input_string.translate(translation_table)\n",
    "    return cleaned_string\n",
    "\n",
    "def print_pattern(text=\"\"):\n",
    "    print(text)\n",
    "    print(\"**************************************************\")\n",
    "    \n",
    "def calculate_sentence_lengths(list_of_sentences):\n",
    "    sentence_lengths_dict={}\n",
    "    for i in range(len(list_of_sentences)):\n",
    "        sentence_lengths_dict[i]=len(list_of_sentences[i].split())\n",
    "    return sentence_lengths_dict\n",
    "\n",
    "\n",
    "def save_csv_details(doc_info,rouge_score):\n",
    "    csv_row=[]\n",
    "    csv_row.append(doc_info.mutation_operator_version)\n",
    "    csv_row.append(doc_info.replacement_operator_version)\n",
    "    csv_row.append(doc_info.algorithm)\n",
    "    csv_row.append(doc_info.IP_population_initialization)\n",
    "    csv_row.append(doc_info.topic_enabled)\n",
    "    csv_row.append(doc_info.sbert_enabled)\n",
    "    csv_row.append(doc_info.run)\n",
    "    csv_row.append(doc_info.article_id)\n",
    "    csv_row.append(doc_info.HDP_topics)\n",
    "    \n",
    "\n",
    "    for k,v in rouge_score.items():\n",
    "        cur_row=csv_row.copy()\n",
    "        cur_row.append(k)\n",
    "        rouge_scores_row_dict=v[\"rouge_score_detail\"]\n",
    "        cur_row.append(rouge_scores_row_dict['rouge1']['precision'])\n",
    "        cur_row.append(rouge_scores_row_dict['rouge1']['recall'])\n",
    "        cur_row.append(rouge_scores_row_dict['rouge1']['fmeasure'])\n",
    "        cur_row.append(rouge_scores_row_dict['rouge2']['precision'])\n",
    "        cur_row.append(rouge_scores_row_dict['rouge2']['recall'])\n",
    "        cur_row.append(rouge_scores_row_dict['rouge2']['fmeasure'])\n",
    "        cur_row.append(rouge_scores_row_dict['rougeL']['precision'])\n",
    "        cur_row.append(rouge_scores_row_dict['rougeL']['recall'])\n",
    "        cur_row.append(rouge_scores_row_dict['rougeL']['fmeasure'])  \n",
    "\n",
    "        with open(doc_info.result_path+doc_info.delim+doc_info.csv_name+'_final_results.csv', 'a') as f_object:\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(cur_row)\n",
    "            f_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.149715Z",
     "iopub.status.idle": "2024-03-17T14:37:07.150077Z",
     "shell.execute_reply": "2024-03-17T14:37:07.149919Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.149905Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_all_files(params,file_paths_class):\n",
    "\n",
    "    result_path=file_paths_class.result_path\n",
    "    dataset_path=file_paths_class.dataset_path\n",
    "    result_file_name=file_paths_class.result_file_name\n",
    "    version=file_paths_class.version\n",
    "    details_path=file_paths_class.details_path\n",
    "    dataset_access_path=file_paths_class.dataset_access_path\n",
    "    \n",
    "    \n",
    "    fulltext_file_names={}\n",
    "    extracts_file_names={}\n",
    "    with open(dataset_path+\"extracts_duc_filenames.pickle\",\"rb\") as file:\n",
    "        extracts_file_names=pickle.load(file)\n",
    "    with open(dataset_path+\"fulltext_duc_filenames.pickle\",\"rb\") as file:\n",
    "        fulltext_file_names=pickle.load(file)\n",
    "    no_of_docs=10\n",
    "    ctr=0\n",
    "    docs_processed=0\n",
    "    while docs_processed<no_of_docs:\n",
    "        print_pattern()\n",
    "        doc_name=list(fulltext_file_names.keys())[ctr]\n",
    "        L=params.L\n",
    "        full_text_file_path=dataset_access_path+fulltext_file_names[doc_name]\n",
    "        greedy_extract_file_path=dataset_access_path+extracts_file_names[doc_name][L][0]\n",
    "        article_id=doc_name\n",
    "        print\n",
    "        print(\"Checking if doc : \",doc_name,\"passes all preprocessing steps ::....\")\n",
    "        if article_id in params.restricted_docs_list:  #int(article_id)<=params.doc_threshold_value or\n",
    "            print_pattern(\"Present in Restricted Docs List....Skipping...\")\n",
    "            ctr+=1\n",
    "            continue\n",
    "        if ctr <= params.doc_threshold_value :\n",
    "            print_pattern(\"Present under Doc Restricted Value.....Skipping....\")\n",
    "            ctr+=1\n",
    "            continue\n",
    "\n",
    "        original_text_content=read_text_file(full_text_file_path)\n",
    "        revised_text_file_content=remove_specials(original_text_content)\n",
    "        cleaned_text=clean_text(revised_text_file_content)\n",
    "        preprocessed_inputs,list_of_sentences_via_preprocess=preprocess(cleaned_text,params.spacy_use)\n",
    "        list_of_sentences=list_of_sentences_via_preprocess.copy()\n",
    "\n",
    "    \n",
    "        greedy_extract_text=read_text_file(greedy_extract_file_path)\n",
    "        greedy_extract_text=remove_specials(greedy_extract_text)\n",
    "        greedy_extract_text=clean_text(greedy_extract_text)\n",
    "        greedy_list_of_sentences=segmentation(greedy_extract_text)\n",
    "        greedy_extract_text=\".\".join(greedy_list_of_sentences)\n",
    "        \n",
    "\n",
    "        if len(list_of_sentences_via_preprocess)==len(preprocessed_inputs):\n",
    "            print(\"Preprocessing Step Cleared Successfully\")\n",
    "        else:\n",
    "            print_pattern(\"Cardinality of Sentences not matched, Please rectify doc structure, Skipping...!!!\")\n",
    "            ctr+=1\n",
    "            continue\n",
    "            raise ValueError(\"Stop!\")\n",
    "        print(\"No of sentences = \",len(preprocessed_inputs))\n",
    "        if len(preprocessed_inputs)>params.sentence_threshold_value:\n",
    "            print_pattern(\"Too Long-Skipping.....\")\n",
    "            ctr+=1\n",
    "            continue\n",
    "        \n",
    "        print(\"Document : \",article_id,\"Processing !!!--------------\")\n",
    "\n",
    "        tfik_dict={}\n",
    "        for i in range(len(preprocessed_inputs)):\n",
    "            tfik_dict[i]=tfik(preprocessed_inputs[i])\n",
    "\n",
    "        no_of_sentences=len(preprocessed_inputs)\n",
    "        terms_dict=distinct_terms(tfik_dict)\n",
    "        no_of_terms=len(list(terms_dict.keys()))\n",
    "\n",
    "        index_to_term_mapper, term_to_index_mapper=indices_terms_mapper(terms_dict)\n",
    "        term_index_sentence_dict=term_indices_of_sentences(preprocessed_inputs,term_to_index_mapper)\n",
    "        if params.sbert_enabled==True:\n",
    "            o_dict,wik_dict=SBERT_embeddings(list_of_sentences)\n",
    "        elif params.sbert_enabled==False:\n",
    "            wik_dict=wik(tfik_dict,terms_dict,index_to_term_mapper,term_to_index_mapper,no_of_sentences,no_of_terms)\n",
    "            o_dict=mean_o(wik_dict,no_of_sentences,no_of_terms)\n",
    "    \n",
    "        topic_assignment_matrix,topic_to_words_mapper,sentence_probabilities,HDP_topics=retrieve_topic_assignment_matrix(list_of_sentences,preprocessed_inputs, params.num_topics,params.n_top_words)\n",
    "        kw_matrix=retrieve_kw_matrix(revised_text_file_content,list_of_sentences,params.num_keywords)\n",
    "        kw_topic_info=kw_topic_information_class(preprocessed_inputs,topic_to_words_mapper,topic_assignment_matrix,kw_matrix,wik_dict,o_dict)    \n",
    "        \n",
    "        tf_scores=sentence_scores_calculation(tfik_dict,term_to_index_mapper,index_to_term_mapper)\n",
    "        textrank_scores=textrank_scores_calculate(wik_dict,list_of_sentences)\n",
    "        compressed_doc_info=compressed_doc_info_class(preprocessed_inputs,list_of_sentences,params.L,params.epsilon)\n",
    "        graph_scores=GETS(compressed_doc_info).gets_sentence_scores\n",
    "        lxr=lexrank_model(dataset_access_path)\n",
    "        lexrankscores_cont = lxr.rank_sentences(list_of_sentences,threshold=None,fast_power_method=False)\n",
    "        lexrankscores_dict={}\n",
    "        for i in range(len(preprocessed_inputs)):\n",
    "            lexrankscores_dict[i]=lexrankscores_cont[i]\n",
    "        lexrankscores_dict = dict(sorted(lexrankscores_dict.items(), key=lambda item: item[1],reverse=True))\n",
    "        scores_details=scores_class(tf_scores,textrank_scores,graph_scores,lexrankscores_dict)\n",
    "        \n",
    "        \n",
    "        sentence_lengths=calculate_sentence_lengths(list_of_sentences)\n",
    "        doc_info=doc_info_class(full_text_file_path,greedy_extract_file_path,original_text_content,L,revised_text_file_content,preprocessed_inputs,list_of_sentences,article_id,tfik_dict,no_of_sentences,terms_dict,no_of_terms,index_to_term_mapper,term_to_index_mapper,wik_dict,o_dict,kw_topic_info,scores_details,sentence_lengths,params,sentence_probabilities,file_paths_class,HDP_topics)\n",
    "        \n",
    "        if doc_info.algorithm==\"MOABC\":\n",
    "            NDS_archive,NDS_archive_tracker,population_tracker=run_MOABC(doc_info)\n",
    "        elif doc_info.algorithm==\"NSGA\":\n",
    "            NDS_archive,NDS_archive_tracker,population_tracker=run_NSGA(doc_info)\n",
    "\n",
    "        all_rouge_scores=extract_single_result(NDS_archive,doc_info) \n",
    "        \n",
    "        rouge_score=rouge_score_calc(greedy_extract_text,list_of_sentences,all_rouge_scores)\n",
    "\n",
    "        add_result_line(result_path+result_file_name,article_id + \":     \"+str(rouge_score))\n",
    "        \n",
    "        save_details(saver_class(doc_info,NDS_archive,greedy_extract_text,rouge_score,NDS_archive_tracker,params,population_tracker),details_path,version,current_datetime_string,article_id)\n",
    "        \n",
    "        save_csv_details(doc_info,rouge_score)\n",
    "    \n",
    "        docs_processed+=1\n",
    "        ctr+=1\n",
    "        print_pattern()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.151324Z",
     "iopub.status.idle": "2024-03-17T14:37:07.152032Z",
     "shell.execute_reply": "2024-03-17T14:37:07.151853Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.151831Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class params_class:\n",
    "    def __init__(self,doc_threshold_value,sentence_threshold_value,spacy_use,restricted_docs_list,epsilon,L,no_of_objectives,pop_size,pm,trial_cutoff_limit,cycles,delta,num_topics,n_top_words,mutation_operator_version,replacement_operator_version,algorithm,IP_population_initialization,topic_enabled,sbert_enabled,num_keywords,Lextract,run,delim,kaggle,csv_name):\n",
    "        self.doc_threshold_value=doc_threshold_value\n",
    "        self.sentence_threshold_value=sentence_threshold_value\n",
    "        self.spacy_use=spacy_use\n",
    "        self.restricted_docs_list=restricted_docs_list\n",
    "        self.epsilon=epsilon\n",
    "        self.L=L\n",
    "        self.no_of_objectives=no_of_objectives\n",
    "        self.pop_size=pop_size\n",
    "        self.pm=pm\n",
    "        self.trial_cutoff_limit=trial_cutoff_limit\n",
    "        self.cycles=cycles\n",
    "        self.delta=delta\n",
    "        self.num_topics=num_topics\n",
    "        self.n_top_words=n_top_words\n",
    "        self.mutation_operator_version=mutation_operator_version\n",
    "        self.replacement_operator_version=replacement_operator_version\n",
    "        self.algorithm=algorithm\n",
    "        self.IP_population_initialization=IP_population_initialization\n",
    "        self.topic_enabled=topic_enabled\n",
    "        self.sbert_enabled=sbert_enabled\n",
    "        self.num_keywords=num_keywords\n",
    "        self.Lextract=Lextract\n",
    "        self.run=run\n",
    "        self.delim=delim\n",
    "        self.kaggle=kaggle\n",
    "        self.csv_name=csv_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-17T14:37:07.156076Z",
     "iopub.status.idle": "2024-03-17T14:37:07.156479Z",
     "shell.execute_reply": "2024-03-17T14:37:07.156311Z",
     "shell.execute_reply.started": "2024-03-17T14:37:07.156296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version In Progress ::: MOABC_epsilon_dominate_MutationOperatorVersion=1_IndependentRun=0_topic-enabled=v3_IP-population-initialization=False_sbert-enabled=False\n",
      "\n",
      "**************************************************\n",
      "Checking if doc :  d061j passes all preprocessing steps ::....\n",
      "Preprocessing Step Cleared Successfully\n",
      "No of sentences =  187\n",
      "Document :  d061j Processing !!!--------------\n",
      "Cycle =  0 Completed, Len of Archive = 22 ,  No of solns added/removed =  22 Time taken :  7.97\n",
      "Cycle =  1 Completed, Len of Archive = 27 ,  No of solns added/removed =  5 Time taken :  5.69\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "Tropical Storm Gilbert formed in the  eastern Caribbean and strengthened into a hurricane Saturday night.Gilbert reached Jamaica after skirting  southern Puerto Rico  Haiti and the Dominican Republic.Hurricane Gilbert  one of the  strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled  thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts  of Cancun and Cozumel.The Mexican National Weather Service  reported winds gusting as high as 218 mph earlier Wednesday with sustained winds  of 179 mph.More than 120 000 people on the  northeast Yucatan coast were evacuated  the Yucatan state government said.Shelters had little or no food  water  or blankets and power was out.The storm killed 19 people in Jamaica  and five in the Dominican Republic before moving west to Mexico.Prime Minister Edward Seaga of Jamaica  said Wednesday the storm destroyed an estimated 100 000 of Jamaicas 500 000  homes when it throttled the island Monday.The National Hurricane Center said a  hurricane watch was in effect on the Texas coast from Brownsville to Port Arthur  and along the coast of northeast Mexico from Tampico north.The National Hurricane Center said  Gilbert was the most intense storm on record in terms of barometric  pressure\n",
      "PREDICTED EXTRACT : \n",
      "Hurricane Gilbert swept toward the Dominican Republic Sunday  and the Civil Defense alerted its heavily populated south coast to prepare for high winds  heavy rains and high seas . The National Weather Service in San Juan  Puerto Rico  said Gilbert was moving westward at 15 mph with a broad area of cloudiness and heavy weather rotating around the center of the storm . Hurricane Gilbert  packing 110 mph winds and torrential rain  moved over this capital city today after skirting Puerto Rico  Haiti and the Dominican Republic . Right now its actually moving over Jamaica  said Bob Sheets  director of the National Hurricane Center in Miami . Its certainly one of the larger systems weve seen in the Caribbean for a long time  said Hal Gerrish  forecaster at the National Hurricane Center . A National Weather Service report said the hurricane was moving west at 17 mph with maximum sustained winds of 115 mph . The National Hurricane Center said Gilbert was the most intense storm on record in terms of barometric pressure . Hurricane Gilbert swept toward Jamaica yesterday with 100mileanhour winds  and officials issued warnings to residents on the southern coasts of the Dominican Republic  Haiti and Cuba . Forecasters said the hurricane was gaining strength as it passed over the ocean and would dump heavy rain on the Dominican Republic and Haiti as it moved south of Hispaniola  the Caribbean island they share  and headed west . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.45454545454545453, 'recall': 0.5147058823529411, 'fmeasure': 0.4827586206896551}, 'rouge2': {'precision': 0.1956521739130435, 'recall': 0.22167487684729065, 'fmeasure': 0.20785219399538105}, 'rougeL': {'precision': 0.23376623376623376, 'recall': 0.2647058823529412, 'fmeasure': 0.24827586206896549}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "Tropical Storm Gilbert formed in the  eastern Caribbean and strengthened into a hurricane Saturday night.Gilbert reached Jamaica after skirting  southern Puerto Rico  Haiti and the Dominican Republic.Hurricane Gilbert  one of the  strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled  thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts  of Cancun and Cozumel.The Mexican National Weather Service  reported winds gusting as high as 218 mph earlier Wednesday with sustained winds  of 179 mph.More than 120 000 people on the  northeast Yucatan coast were evacuated  the Yucatan state government said.Shelters had little or no food  water  or blankets and power was out.The storm killed 19 people in Jamaica  and five in the Dominican Republic before moving west to Mexico.Prime Minister Edward Seaga of Jamaica  said Wednesday the storm destroyed an estimated 100 000 of Jamaicas 500 000  homes when it throttled the island Monday.The National Hurricane Center said a  hurricane watch was in effect on the Texas coast from Brownsville to Port Arthur  and along the coast of northeast Mexico from Tampico north.The National Hurricane Center said  Gilbert was the most intense storm on record in terms of barometric  pressure\n",
      "PREDICTED EXTRACT : \n",
      "Hurricane Gilbert swept toward the Dominican Republic Sunday  and the Civil Defense alerted its heavily populated south coast to prepare for high winds  heavy rains and high seas . The National Weather Service in San Juan  Puerto Rico  said Gilbert was moving westward at 15 mph with a broad area of cloudiness and heavy weather rotating around the center of the storm . It looks like the eye is going to move lengthwise across that island  and theyre going to bear the full brunt of this powerful hurricane  Sheets said . Its certainly one of the larger systems weve seen in the Caribbean for a long time  said Hal Gerrish  forecaster at the National Hurricane Center . Prime Minister Edward Seaga of Jamaica alerted all government agencies  saying Sunday night  Hurricane Gilbert appears to be a real threat and everyone should follow the instructions and hurricane precautions issued by the Office of Disaster Preparedness in order to minimize the danger . In the report from Havana received in Mexico City  Prensa Latina said civil defense officials were broadcasting bulletins on national radio and television recommending emergency measures and providing information on the storm . Heavy rain and stiff winds downed power lines and caused flooding in the Dominican Republic on Sunday night as the hurricanes center passed just south of the Barahona peninsula  then less than 100 miles from neighboring Haiti . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.4488888888888889, 'recall': 0.4950980392156863, 'fmeasure': 0.47086247086247085}, 'rouge2': {'precision': 0.12946428571428573, 'recall': 0.14285714285714285, 'fmeasure': 0.13583138173302106}, 'rougeL': {'precision': 0.18222222222222223, 'recall': 0.20098039215686275, 'fmeasure': 0.19114219114219114}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "Tropical Storm Gilbert formed in the  eastern Caribbean and strengthened into a hurricane Saturday night.Gilbert reached Jamaica after skirting  southern Puerto Rico  Haiti and the Dominican Republic.Hurricane Gilbert  one of the  strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled  thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts  of Cancun and Cozumel.The Mexican National Weather Service  reported winds gusting as high as 218 mph earlier Wednesday with sustained winds  of 179 mph.More than 120 000 people on the  northeast Yucatan coast were evacuated  the Yucatan state government said.Shelters had little or no food  water  or blankets and power was out.The storm killed 19 people in Jamaica  and five in the Dominican Republic before moving west to Mexico.Prime Minister Edward Seaga of Jamaica  said Wednesday the storm destroyed an estimated 100 000 of Jamaicas 500 000  homes when it throttled the island Monday.The National Hurricane Center said a  hurricane watch was in effect on the Texas coast from Brownsville to Port Arthur  and along the coast of northeast Mexico from Tampico north.The National Hurricane Center said  Gilbert was the most intense storm on record in terms of barometric  pressure\n",
      "PREDICTED EXTRACT : \n",
      "Hurricane Gilbert swept toward the Dominican Republic Sunday  and the Civil Defense alerted its heavily populated south coast to prepare for high winds  heavy rains and high seas . The storm was approaching from the southeast with sustained winds of 75 mph gusting to 92 mph . The National Hurricane Center in Miami reported its position at 2 a . Hurricane Gilbert  packing 110 mph winds and torrential rain  moved over this capital city today after skirting Puerto Rico  Haiti and the Dominican Republic . Right now its actually moving over Jamaica  said Bob Sheets  director of the National Hurricane Center in Miami . Its certainly one of the larger systems weve seen in the Caribbean for a long time  said Hal Gerrish  forecaster at the National Hurricane Center . The storm ripped the roofs off houses and flooded coastal areas of southwestern Puerto Rico after reaching hurricane strength off the islands southeast Saturday night . A National Weather Service report said the hurricane was moving west at 17 mph with maximum sustained winds of 115 mph . The National Hurricane Center said Gilbert was the most intense storm on record in terms of barometric pressure . It reached tropical storm status by Saturday and a hurricane Sunday . When sustained winds reach 39 mph  the system becomes a named tropical storm . It reaches hurricane status when sustained winds hit 74 mph . The forecasters said the Dominican Republic would get as much as 10 inches of rain yesterday  with similar amounts falling in Haiti last night and tonight . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.4475806451612903, 'recall': 0.5441176470588235, 'fmeasure': 0.49115044247787604}, 'rouge2': {'precision': 0.20242914979757085, 'recall': 0.24630541871921183, 'fmeasure': 0.22222222222222224}, 'rougeL': {'precision': 0.21370967741935484, 'recall': 0.25980392156862747, 'fmeasure': 0.2345132743362832}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "Tropical Storm Gilbert formed in the  eastern Caribbean and strengthened into a hurricane Saturday night.Gilbert reached Jamaica after skirting  southern Puerto Rico  Haiti and the Dominican Republic.Hurricane Gilbert  one of the  strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled  thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts  of Cancun and Cozumel.The Mexican National Weather Service  reported winds gusting as high as 218 mph earlier Wednesday with sustained winds  of 179 mph.More than 120 000 people on the  northeast Yucatan coast were evacuated  the Yucatan state government said.Shelters had little or no food  water  or blankets and power was out.The storm killed 19 people in Jamaica  and five in the Dominican Republic before moving west to Mexico.Prime Minister Edward Seaga of Jamaica  said Wednesday the storm destroyed an estimated 100 000 of Jamaicas 500 000  homes when it throttled the island Monday.The National Hurricane Center said a  hurricane watch was in effect on the Texas coast from Brownsville to Port Arthur  and along the coast of northeast Mexico from Tampico north.The National Hurricane Center said  Gilbert was the most intense storm on record in terms of barometric  pressure\n",
      "PREDICTED EXTRACT : \n",
      "Hurricane Gilbert swept toward the Dominican Republic Sunday  and the Civil Defense alerted its heavily populated south coast to prepare for high winds  heavy rains and high seas . The National Weather Service in San Juan  Puerto Rico  said Gilbert was moving westward at 15 mph with a broad area of cloudiness and heavy weather rotating around the center of the storm . It looks like the eye is going to move lengthwise across that island  and theyre going to bear the full brunt of this powerful hurricane  Sheets said . Its certainly one of the larger systems weve seen in the Caribbean for a long time  said Hal Gerrish  forecaster at the National Hurricane Center . Prime Minister Edward Seaga of Jamaica alerted all government agencies  saying Sunday night  Hurricane Gilbert appears to be a real threat and everyone should follow the instructions and hurricane precautions issued by the Office of Disaster Preparedness in order to minimize the danger . In the report from Havana received in Mexico City  Prensa Latina said civil defense officials were broadcasting bulletins on national radio and television recommending emergency measures and providing information on the storm . Heavy rain and stiff winds downed power lines and caused flooding in the Dominican Republic on Sunday night as the hurricanes center passed just south of the Barahona peninsula  then less than 100 miles from neighboring Haiti . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.4488888888888889, 'recall': 0.4950980392156863, 'fmeasure': 0.47086247086247085}, 'rouge2': {'precision': 0.12946428571428573, 'recall': 0.14285714285714285, 'fmeasure': 0.13583138173302106}, 'rougeL': {'precision': 0.18222222222222223, 'recall': 0.20098039215686275, 'fmeasure': 0.19114219114219114}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "Tropical Storm Gilbert formed in the  eastern Caribbean and strengthened into a hurricane Saturday night.Gilbert reached Jamaica after skirting  southern Puerto Rico  Haiti and the Dominican Republic.Hurricane Gilbert  one of the  strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled  thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts  of Cancun and Cozumel.The Mexican National Weather Service  reported winds gusting as high as 218 mph earlier Wednesday with sustained winds  of 179 mph.More than 120 000 people on the  northeast Yucatan coast were evacuated  the Yucatan state government said.Shelters had little or no food  water  or blankets and power was out.The storm killed 19 people in Jamaica  and five in the Dominican Republic before moving west to Mexico.Prime Minister Edward Seaga of Jamaica  said Wednesday the storm destroyed an estimated 100 000 of Jamaicas 500 000  homes when it throttled the island Monday.The National Hurricane Center said a  hurricane watch was in effect on the Texas coast from Brownsville to Port Arthur  and along the coast of northeast Mexico from Tampico north.The National Hurricane Center said  Gilbert was the most intense storm on record in terms of barometric  pressure\n",
      "PREDICTED EXTRACT : \n",
      "Hurricane Gilbert  packing 110 mph winds and torrential rain  moved over this capital city today after skirting Puerto Rico  Haiti and the Dominican Republic . People were running around in the main lobby of our hotel on Grand Cayman Island like chickens with their heads cut off  said one man . A National Weather Service report said the hurricane was moving west at 17 mph with maximum sustained winds of 115 mph . It looks like the eye is going to move lengthwise across that island  and theyre going to bear the full brunt of this powerful hurricane  he said . Havana Radio  meanwhile  reported Monday that 25 000 people were evacuated from coastal areas in Guantanamo Province on the nations southeastern coast as Gilberts winds and rain began to brush the island . Hurricane Gilbert  one of the strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts of Cancun and Cozumel . Prime Minister Edward Seaga of Jamaica said Wednesday the storm destroyed an estimated 100 000 of Jamaicas 500 000 homes when it throttled the island Monday . Hurricane Gilbert swept toward Jamaica yesterday with 100mileanhour winds  and officials issued warnings to residents on the southern coasts of the Dominican Republic  Haiti and Cuba . EDT  the center of the hurricane was about 100 miles south of the Dominican Republic and 425 miles east of Kingston  Jamaica . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.5720338983050848, 'recall': 0.6617647058823529, 'fmeasure': 0.6136363636363638}, 'rouge2': {'precision': 0.33617021276595743, 'recall': 0.3891625615763547, 'fmeasure': 0.36073059360730597}, 'rougeL': {'precision': 0.3474576271186441, 'recall': 0.4019607843137255, 'fmeasure': 0.37272727272727274}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "Tropical Storm Gilbert formed in the  eastern Caribbean and strengthened into a hurricane Saturday night.Gilbert reached Jamaica after skirting  southern Puerto Rico  Haiti and the Dominican Republic.Hurricane Gilbert  one of the  strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled  thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts  of Cancun and Cozumel.The Mexican National Weather Service  reported winds gusting as high as 218 mph earlier Wednesday with sustained winds  of 179 mph.More than 120 000 people on the  northeast Yucatan coast were evacuated  the Yucatan state government said.Shelters had little or no food  water  or blankets and power was out.The storm killed 19 people in Jamaica  and five in the Dominican Republic before moving west to Mexico.Prime Minister Edward Seaga of Jamaica  said Wednesday the storm destroyed an estimated 100 000 of Jamaicas 500 000  homes when it throttled the island Monday.The National Hurricane Center said a  hurricane watch was in effect on the Texas coast from Brownsville to Port Arthur  and along the coast of northeast Mexico from Tampico north.The National Hurricane Center said  Gilbert was the most intense storm on record in terms of barometric  pressure\n",
      "PREDICTED EXTRACT : \n",
      "Hurricane Gilbert swept toward the Dominican Republic Sunday  and the Civil Defense alerted its heavily populated south coast to prepare for high winds  heavy rains and high seas . The National Weather Service in San Juan  Puerto Rico  said Gilbert was moving westward at 15 mph with a broad area of cloudiness and heavy weather rotating around the center of the storm . It looks like the eye is going to move lengthwise across that island  and theyre going to bear the full brunt of this powerful hurricane  Sheets said . Its certainly one of the larger systems weve seen in the Caribbean for a long time  said Hal Gerrish  forecaster at the National Hurricane Center . Prime Minister Edward Seaga of Jamaica alerted all government agencies  saying Sunday night  Hurricane Gilbert appears to be a real threat and everyone should follow the instructions and hurricane precautions issued by the Office of Disaster Preparedness in order to minimize the danger . In the report from Havana received in Mexico City  Prensa Latina said civil defense officials were broadcasting bulletins on national radio and television recommending emergency measures and providing information on the storm . Heavy rain and stiff winds downed power lines and caused flooding in the Dominican Republic on Sunday night as the hurricanes center passed just south of the Barahona peninsula  then less than 100 miles from neighboring Haiti . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.4488888888888889, 'recall': 0.4950980392156863, 'fmeasure': 0.47086247086247085}, 'rouge2': {'precision': 0.12946428571428573, 'recall': 0.14285714285714285, 'fmeasure': 0.13583138173302106}, 'rougeL': {'precision': 0.18222222222222223, 'recall': 0.20098039215686275, 'fmeasure': 0.19114219114219114}}\n",
      "\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "Tropical Storm Gilbert formed in the  eastern Caribbean and strengthened into a hurricane Saturday night.Gilbert reached Jamaica after skirting  southern Puerto Rico  Haiti and the Dominican Republic.Hurricane Gilbert  one of the  strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled  thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts  of Cancun and Cozumel.The Mexican National Weather Service  reported winds gusting as high as 218 mph earlier Wednesday with sustained winds  of 179 mph.More than 120 000 people on the  northeast Yucatan coast were evacuated  the Yucatan state government said.Shelters had little or no food  water  or blankets and power was out.The storm killed 19 people in Jamaica  and five in the Dominican Republic before moving west to Mexico.Prime Minister Edward Seaga of Jamaica  said Wednesday the storm destroyed an estimated 100 000 of Jamaicas 500 000  homes when it throttled the island Monday.The National Hurricane Center said a  hurricane watch was in effect on the Texas coast from Brownsville to Port Arthur  and along the coast of northeast Mexico from Tampico north.The National Hurricane Center said  Gilbert was the most intense storm on record in terms of barometric  pressure\n",
      "PREDICTED EXTRACT : \n",
      "Hurricane Gilbert  packing 110 mph winds and torrential rain  moved over this capital city today after skirting Puerto Rico  Haiti and the Dominican Republic . Right now its actually moving over Jamaica  said Bob Sheets  director of the National Hurricane Center in Miami . Its certainly one of the larger systems weve seen in the Caribbean for a long time  said Hal Gerrish  forecaster at the National Hurricane Center . Heavy rain and stiff winds downed power lines and caused flooding in the Dominican Republic on Sunday night as the hurricanes center passed just south of the Barahona peninsula  then less than 100 miles from neighboring Haiti . Havana Radio  meanwhile  reported Monday that 25 000 people were evacuated from coastal areas in Guantanamo Province on the nations southeastern coast as Gilberts winds and rain began to brush the island . As Gilbert moved away from the Yucatan Peninsula Wednesday night  the hurricane formed a double eye  two concentric circles of thunderstorms often characteristic of a strong storm that has crossed land and is moving over the water again . The National Hurricane Center said a hurricane watch was in effect on the Texas coast from Brownsville to Port Arthur and along the coast of northeast Mexico from Tampico north . Forecasters said the hurricane was gaining strength as it passed over the ocean and would dump heavy rain on the Dominican Republic and Haiti as it moved south of Hispaniola  the Caribbean island they share  and headed west . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.4834710743801653, 'recall': 0.5735294117647058, 'fmeasure': 0.5246636771300448}, 'rouge2': {'precision': 0.22821576763485477, 'recall': 0.270935960591133, 'fmeasure': 0.24774774774774777}, 'rougeL': {'precision': 0.29338842975206614, 'recall': 0.3480392156862745, 'fmeasure': 0.3183856502242152}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "Tropical Storm Gilbert formed in the  eastern Caribbean and strengthened into a hurricane Saturday night.Gilbert reached Jamaica after skirting  southern Puerto Rico  Haiti and the Dominican Republic.Hurricane Gilbert  one of the  strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled  thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts  of Cancun and Cozumel.The Mexican National Weather Service  reported winds gusting as high as 218 mph earlier Wednesday with sustained winds  of 179 mph.More than 120 000 people on the  northeast Yucatan coast were evacuated  the Yucatan state government said.Shelters had little or no food  water  or blankets and power was out.The storm killed 19 people in Jamaica  and five in the Dominican Republic before moving west to Mexico.Prime Minister Edward Seaga of Jamaica  said Wednesday the storm destroyed an estimated 100 000 of Jamaicas 500 000  homes when it throttled the island Monday.The National Hurricane Center said a  hurricane watch was in effect on the Texas coast from Brownsville to Port Arthur  and along the coast of northeast Mexico from Tampico north.The National Hurricane Center said  Gilbert was the most intense storm on record in terms of barometric  pressure\n",
      "PREDICTED EXTRACT : \n",
      "Hurricane Gilbert swept toward the Dominican Republic Sunday  and the Civil Defense alerted its heavily populated south coast to prepare for high winds  heavy rains and high seas . Meanwhile  Havana Radio reported today that 25 000 people were evacuated from Guantanamo Province on Cubas southeastern coast as strong winds fanning out from Gilbert began brushing the island . Forecasters said the hurricane had been gaining strength as it passed over the ocean after it dumped 5 to 10 inches of rain on the Dominican Republic and Haiti  which share the island of Hispaniola . A National Weather Service report said the hurricane was moving west at 17 mph with maximum sustained winds of 115 mph . Hurricane warnings were issued Monday for the south coast of Cuba east of Camaguey  the Cayman Islands  and Haiti  while warnings were discontinued for the Dominican Republic . By Wednesday night the National Hurricane Center downgraded it to a Category 4  but center director Bob Sheets said Theres no question itll strengthen again once it comes off the Yucatan Peninsula and gets back in open water . One eye was about eight miles wide  and the second about 25 miles wide  said hurricane center meteorologist Jesse Moore . The peninsula ports of Campeche  Celestum  Progreso  Sinzal  Ucaltepen  TelChac  Cancun  Puerto Morelos  and Ciudad del Carmen were closed  the government news agency Notimex said . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.4484304932735426, 'recall': 0.49019607843137253, 'fmeasure': 0.468384074941452}, 'rouge2': {'precision': 0.10810810810810811, 'recall': 0.11822660098522167, 'fmeasure': 0.11294117647058823}, 'rougeL': {'precision': 0.18834080717488788, 'recall': 0.20588235294117646, 'fmeasure': 0.1967213114754098}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "Tropical Storm Gilbert formed in the  eastern Caribbean and strengthened into a hurricane Saturday night.Gilbert reached Jamaica after skirting  southern Puerto Rico  Haiti and the Dominican Republic.Hurricane Gilbert  one of the  strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled  thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts  of Cancun and Cozumel.The Mexican National Weather Service  reported winds gusting as high as 218 mph earlier Wednesday with sustained winds  of 179 mph.More than 120 000 people on the  northeast Yucatan coast were evacuated  the Yucatan state government said.Shelters had little or no food  water  or blankets and power was out.The storm killed 19 people in Jamaica  and five in the Dominican Republic before moving west to Mexico.Prime Minister Edward Seaga of Jamaica  said Wednesday the storm destroyed an estimated 100 000 of Jamaicas 500 000  homes when it throttled the island Monday.The National Hurricane Center said a  hurricane watch was in effect on the Texas coast from Brownsville to Port Arthur  and along the coast of northeast Mexico from Tampico north.The National Hurricane Center said  Gilbert was the most intense storm on record in terms of barometric  pressure\n",
      "PREDICTED EXTRACT : \n",
      "Hurricane Gilbert  packing 110 mph winds and torrential rain  moved over this capital city today after skirting Puerto Rico  Haiti and the Dominican Republic . In the report from Havana received in Mexico City  Prensa Latina said civil defense officials were broadcasting bulletins on national radio and television recommending emergency measures and providing information on the storm . People were running around in the main lobby of our hotel on Grand Cayman Island like chickens with their heads cut off  said one man . A National Weather Service report said the hurricane was moving west at 17 mph with maximum sustained winds of 115 mph . Hurricane Gilbert  one of the strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts of Cancun and Cozumel . The storm killed 19 people in Jamaica and five in the Dominican Republic before moving west to Mexico . Public buildings in Cancun were used as shelters  said Cecila Lavalle  a spokesman for Quintana Roo state government in Chetumal  155 miles southeast of Cozumel . The National Hurricane Center said Gilbert was the most intense storm on record in terms of barometric pressure . It reached tropical storm status by Saturday and a hurricane Sunday . Forecasters said the hurricane was gaining strength as it passed over the ocean and would dump heavy rain on the Dominican Republic and Haiti as it moved south of Hispaniola  the Caribbean island they share  and headed west . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.5551020408163265, 'recall': 0.6666666666666666, 'fmeasure': 0.6057906458797327}, 'rouge2': {'precision': 0.3524590163934426, 'recall': 0.4236453201970443, 'fmeasure': 0.38478747203579416}, 'rougeL': {'precision': 0.34285714285714286, 'recall': 0.4117647058823529, 'fmeasure': 0.3741648106904232}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "Tropical Storm Gilbert formed in the  eastern Caribbean and strengthened into a hurricane Saturday night.Gilbert reached Jamaica after skirting  southern Puerto Rico  Haiti and the Dominican Republic.Hurricane Gilbert  one of the  strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled  thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts  of Cancun and Cozumel.The Mexican National Weather Service  reported winds gusting as high as 218 mph earlier Wednesday with sustained winds  of 179 mph.More than 120 000 people on the  northeast Yucatan coast were evacuated  the Yucatan state government said.Shelters had little or no food  water  or blankets and power was out.The storm killed 19 people in Jamaica  and five in the Dominican Republic before moving west to Mexico.Prime Minister Edward Seaga of Jamaica  said Wednesday the storm destroyed an estimated 100 000 of Jamaicas 500 000  homes when it throttled the island Monday.The National Hurricane Center said a  hurricane watch was in effect on the Texas coast from Brownsville to Port Arthur  and along the coast of northeast Mexico from Tampico north.The National Hurricane Center said  Gilbert was the most intense storm on record in terms of barometric  pressure\n",
      "PREDICTED EXTRACT : \n",
      "Hurricane Gilbert  packing 110 mph winds and torrential rain  moved over this capital city today after skirting Puerto Rico  Haiti and the Dominican Republic . In the report from Havana received in Mexico City  Prensa Latina said civil defense officials were broadcasting bulletins on national radio and television recommending emergency measures and providing information on the storm . People were running around in the main lobby of our hotel on Grand Cayman Island like chickens with their heads cut off  said one man . A National Weather Service report said the hurricane was moving west at 17 mph with maximum sustained winds of 115 mph . Hurricane Gilbert  one of the strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts of Cancun and Cozumel . The storm killed 19 people in Jamaica and five in the Dominican Republic before moving west to Mexico . Public buildings in Cancun were used as shelters  said Cecila Lavalle  a spokesman for Quintana Roo state government in Chetumal  155 miles southeast of Cozumel . The National Hurricane Center said Gilbert was the most intense storm on record in terms of barometric pressure . It reached tropical storm status by Saturday and a hurricane Sunday . Forecasters said the hurricane was gaining strength as it passed over the ocean and would dump heavy rain on the Dominican Republic and Haiti as it moved south of Hispaniola  the Caribbean island they share  and headed west . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.5551020408163265, 'recall': 0.6666666666666666, 'fmeasure': 0.6057906458797327}, 'rouge2': {'precision': 0.3524590163934426, 'recall': 0.4236453201970443, 'fmeasure': 0.38478747203579416}, 'rougeL': {'precision': 0.34285714285714286, 'recall': 0.4117647058823529, 'fmeasure': 0.3741648106904232}}\n",
      "\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "Tropical Storm Gilbert formed in the  eastern Caribbean and strengthened into a hurricane Saturday night.Gilbert reached Jamaica after skirting  southern Puerto Rico  Haiti and the Dominican Republic.Hurricane Gilbert  one of the  strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled  thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts  of Cancun and Cozumel.The Mexican National Weather Service  reported winds gusting as high as 218 mph earlier Wednesday with sustained winds  of 179 mph.More than 120 000 people on the  northeast Yucatan coast were evacuated  the Yucatan state government said.Shelters had little or no food  water  or blankets and power was out.The storm killed 19 people in Jamaica  and five in the Dominican Republic before moving west to Mexico.Prime Minister Edward Seaga of Jamaica  said Wednesday the storm destroyed an estimated 100 000 of Jamaicas 500 000  homes when it throttled the island Monday.The National Hurricane Center said a  hurricane watch was in effect on the Texas coast from Brownsville to Port Arthur  and along the coast of northeast Mexico from Tampico north.The National Hurricane Center said  Gilbert was the most intense storm on record in terms of barometric  pressure\n",
      "PREDICTED EXTRACT : \n",
      "Hurricane Gilbert  packing 110 mph winds and torrential rain  moved over this capital city today after skirting Puerto Rico  Haiti and the Dominican Republic . In the report from Havana received in Mexico City  Prensa Latina said civil defense officials were broadcasting bulletins on national radio and television recommending emergency measures and providing information on the storm . People were running around in the main lobby of our hotel on Grand Cayman Island like chickens with their heads cut off  said one man . A National Weather Service report said the hurricane was moving west at 17 mph with maximum sustained winds of 115 mph . Hurricane Gilbert  one of the strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts of Cancun and Cozumel . The storm killed 19 people in Jamaica and five in the Dominican Republic before moving west to Mexico . Public buildings in Cancun were used as shelters  said Cecila Lavalle  a spokesman for Quintana Roo state government in Chetumal  155 miles southeast of Cozumel . The National Hurricane Center said Gilbert was the most intense storm on record in terms of barometric pressure . It reached tropical storm status by Saturday and a hurricane Sunday . Forecasters said the hurricane was gaining strength as it passed over the ocean and would dump heavy rain on the Dominican Republic and Haiti as it moved south of Hispaniola  the Caribbean island they share  and headed west . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.5551020408163265, 'recall': 0.6666666666666666, 'fmeasure': 0.6057906458797327}, 'rouge2': {'precision': 0.3524590163934426, 'recall': 0.4236453201970443, 'fmeasure': 0.38478747203579416}, 'rougeL': {'precision': 0.34285714285714286, 'recall': 0.4117647058823529, 'fmeasure': 0.3741648106904232}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "Tropical Storm Gilbert formed in the  eastern Caribbean and strengthened into a hurricane Saturday night.Gilbert reached Jamaica after skirting  southern Puerto Rico  Haiti and the Dominican Republic.Hurricane Gilbert  one of the  strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled  thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts  of Cancun and Cozumel.The Mexican National Weather Service  reported winds gusting as high as 218 mph earlier Wednesday with sustained winds  of 179 mph.More than 120 000 people on the  northeast Yucatan coast were evacuated  the Yucatan state government said.Shelters had little or no food  water  or blankets and power was out.The storm killed 19 people in Jamaica  and five in the Dominican Republic before moving west to Mexico.Prime Minister Edward Seaga of Jamaica  said Wednesday the storm destroyed an estimated 100 000 of Jamaicas 500 000  homes when it throttled the island Monday.The National Hurricane Center said a  hurricane watch was in effect on the Texas coast from Brownsville to Port Arthur  and along the coast of northeast Mexico from Tampico north.The National Hurricane Center said  Gilbert was the most intense storm on record in terms of barometric  pressure\n",
      "PREDICTED EXTRACT : \n",
      "Hurricane Gilbert  packing 110 mph winds and torrential rain  moved over this capital city today after skirting Puerto Rico  Haiti and the Dominican Republic . In the report from Havana received in Mexico City  Prensa Latina said civil defense officials were broadcasting bulletins on national radio and television recommending emergency measures and providing information on the storm . People were running around in the main lobby of our hotel on Grand Cayman Island like chickens with their heads cut off  said one man . A National Weather Service report said the hurricane was moving west at 17 mph with maximum sustained winds of 115 mph . Hurricane Gilbert  one of the strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts of Cancun and Cozumel . The storm killed 19 people in Jamaica and five in the Dominican Republic before moving west to Mexico . Public buildings in Cancun were used as shelters  said Cecila Lavalle  a spokesman for Quintana Roo state government in Chetumal  155 miles southeast of Cozumel . The National Hurricane Center said Gilbert was the most intense storm on record in terms of barometric pressure . It reached tropical storm status by Saturday and a hurricane Sunday . Forecasters said the hurricane was gaining strength as it passed over the ocean and would dump heavy rain on the Dominican Republic and Haiti as it moved south of Hispaniola  the Caribbean island they share  and headed west . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.5551020408163265, 'recall': 0.6666666666666666, 'fmeasure': 0.6057906458797327}, 'rouge2': {'precision': 0.3524590163934426, 'recall': 0.4236453201970443, 'fmeasure': 0.38478747203579416}, 'rougeL': {'precision': 0.34285714285714286, 'recall': 0.4117647058823529, 'fmeasure': 0.3741648106904232}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "Tropical Storm Gilbert formed in the  eastern Caribbean and strengthened into a hurricane Saturday night.Gilbert reached Jamaica after skirting  southern Puerto Rico  Haiti and the Dominican Republic.Hurricane Gilbert  one of the  strongest storms ever  slammed into the Yucatan Peninsula Wednesday and leveled  thatched homes  tore off roofs  uprooted trees and cut off the Caribbean resorts  of Cancun and Cozumel.The Mexican National Weather Service  reported winds gusting as high as 218 mph earlier Wednesday with sustained winds  of 179 mph.More than 120 000 people on the  northeast Yucatan coast were evacuated  the Yucatan state government said.Shelters had little or no food  water  or blankets and power was out.The storm killed 19 people in Jamaica  and five in the Dominican Republic before moving west to Mexico.Prime Minister Edward Seaga of Jamaica  said Wednesday the storm destroyed an estimated 100 000 of Jamaicas 500 000  homes when it throttled the island Monday.The National Hurricane Center said a  hurricane watch was in effect on the Texas coast from Brownsville to Port Arthur  and along the coast of northeast Mexico from Tampico north.The National Hurricane Center said  Gilbert was the most intense storm on record in terms of barometric  pressure\n",
      "PREDICTED EXTRACT : \n",
      "Hurricane Gilbert swept toward the Dominican Republic Sunday  and the Civil Defense alerted its heavily populated south coast to prepare for high winds  heavy rains and high seas . Hurricane Gilbert  packing 110 mph winds and torrential rain  moved over this capital city today after skirting Puerto Rico  Haiti and the Dominican Republic . Meanwhile  Havana Radio reported today that 25 000 people were evacuated from Guantanamo Province on Cubas southeastern coast as strong winds fanning out from Gilbert began brushing the island . Its certainly one of the larger systems weve seen in the Caribbean for a long time  said Hal Gerrish  forecaster at the National Hurricane Center . A National Weather Service report said the hurricane was moving west at 17 mph with maximum sustained winds of 115 mph . The storm killed 19 people in Jamaica and five in the Dominican Republic before moving west to Mexico . One eye was about eight miles wide  and the second about 25 miles wide  said hurricane center meteorologist Jesse Moore . Hurricane Gilbert swept toward Jamaica yesterday with 100mileanhour winds  and officials issued warnings to residents on the southern coasts of the Dominican Republic  Haiti and Cuba . Forecasters said the hurricane was gaining strength as it passed over the ocean and would dump heavy rain on the Dominican Republic and Haiti as it moved south of Hispaniola  the Caribbean island they share  and headed west . Tropical Storm Gilbert formed in the eastern Caribbean and strengthened into a hurricane Saturday night . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.47540983606557374, 'recall': 0.5686274509803921, 'fmeasure': 0.5178571428571428}, 'rouge2': {'precision': 0.2345679012345679, 'recall': 0.28078817733990147, 'fmeasure': 0.2556053811659193}, 'rougeL': {'precision': 0.2581967213114754, 'recall': 0.3088235294117647, 'fmeasure': 0.28125000000000006}}\n",
      "\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "Checking if doc :  d062j passes all preprocessing steps ::....\n",
      "Preprocessing Step Cleared Successfully\n",
      "No of sentences =  131\n",
      "Document :  d062j Processing !!!--------------\n",
      "Cycle =  0 Completed, Len of Archive = 16 ,  No of solns added/removed =  16 Time taken :  3.47\n",
      "Cycle =  1 Completed, Len of Archive = 21 ,  No of solns added/removed =  5 Time taken :  3.16\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "THE BAY AREA QUAKE  In the aftermath of the California  earthquake  President Bush and his aides flew into a whirlwind of earthquake related activity yesterday morning.Some of it was necessary to get  federal help flowing to victims  but some seemed designed mostly to project an  image of a White House in action.With Congress already moving to grant  up to 3 billion in relief  earthquake victims need to know that state officials  are prepared to come up with the states share of disaster and reconstruction  aid as soon as possible  and as soon as is practical.Roads and bridges in the Bay area  appear to have suffered some of the most costly damage.In the Bay Area  that includes  hundreds of thousands of commuters who did not suffer personal damage  but face  massive obstacles in getting to work.He is estimating this weeks disaster  will generate insured losses of 2 billion to 4 billion  following about 4  billion in costs to insurers from Hurricane Hugo.The organization estimated residential  damages from Tuesdays earthquake at 500 million in the Bay area  with between  100 million to 150 million insured\n",
      "PREDICTED EXTRACT : \n",
      "The Association of California Insurance Cos . in Sacramento said that in the San Francisco area roughly 25 to 30 percent of the homes have earthquake insurance . Industry analysts predicted insurers would be able to reverse three years of declining rates and win rate hikes from state regulators due to the quake damages and the estimated 4 billion in damages from Hurricane Hugo  which hammered South Carolina and other parts of the southeastern United States earlier this month . With Congress already moving to grant up to 3 billion  in relief  earthquake victims need to know that state officials are prepared to  come up with the states share of disaster and reconstruction aid as soon as  possible  and as soon as is practical . The earthquakes toll  including possible deep structural damage  goes far beyond the more easily observed damage from a hurricane  says George Reider  a vice president in Aetna Life amp  Casualty Insurance Co . the largest home and auto insurer in California  believes the losses from the earthquake could be somewhat less than the 475 million in damages it expects to pay out for claims resulting from Hurricane Hugo . State Farm said about 25 of its policyholders in California have also purchased earthquake insurance . The Association of California Insurance Companies estimated damage to residential property could total 500 million  but only 100 million to 150 million is insured  it said . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.4890829694323144, 'recall': 0.5925925925925926, 'fmeasure': 0.5358851674641147}, 'rouge2': {'precision': 0.25, 'recall': 0.30319148936170215, 'fmeasure': 0.27403846153846156}, 'rougeL': {'precision': 0.3318777292576419, 'recall': 0.4021164021164021, 'fmeasure': 0.36363636363636365}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "THE BAY AREA QUAKE  In the aftermath of the California  earthquake  President Bush and his aides flew into a whirlwind of earthquake related activity yesterday morning.Some of it was necessary to get  federal help flowing to victims  but some seemed designed mostly to project an  image of a White House in action.With Congress already moving to grant  up to 3 billion in relief  earthquake victims need to know that state officials  are prepared to come up with the states share of disaster and reconstruction  aid as soon as possible  and as soon as is practical.Roads and bridges in the Bay area  appear to have suffered some of the most costly damage.In the Bay Area  that includes  hundreds of thousands of commuters who did not suffer personal damage  but face  massive obstacles in getting to work.He is estimating this weeks disaster  will generate insured losses of 2 billion to 4 billion  following about 4  billion in costs to insurers from Hurricane Hugo.The organization estimated residential  damages from Tuesdays earthquake at 500 million in the Bay area  with between  100 million to 150 million insured\n",
      "PREDICTED EXTRACT : \n",
      "Insured homeowners without earthquake protection will get reimbursed only if their homes were ravaged by fire  which is covered under basic homeowner insurance polices  said Hugh Strawn  director of catastrophe services at the Property Loss Research Bureau in Schaumburg  Ill . Industry analysts predicted insurers would be able to reverse three years of declining rates and win rate hikes from state regulators due to the quake damages and the estimated 4 billion in damages from Hurricane Hugo  which hammered South Carolina and other parts of the southeastern United States earlier this month . Roughly twothirds of all insurance policies written in the United States are sold in California  where many of the nations most damaging earthquakes have occurred . While Deukmejian is not necessarily blaming Caltrans  he seems to  be preparing himself for such investigatory questions as  What did you know  and when did you know it  P P The governors answer  which he has been repeating ever since the day of the  quake  was that no one in Caltrans ever told him a freeway might collapse in an  earthquake . But no  one who has lived in California more than a few months would rule out the  chance that any bridge or freeway could collapse in the right kind of  earthquake . But investors are betting that the financial and psychological impact of the earthquake  coming so soon after the hurricane  will help stem more than two years of intense pricecutting wars among business insurers . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.35269709543568467, 'recall': 0.4497354497354497, 'fmeasure': 0.39534883720930236}, 'rouge2': {'precision': 0.041666666666666664, 'recall': 0.05319148936170213, 'fmeasure': 0.04672897196261682}, 'rougeL': {'precision': 0.11203319502074689, 'recall': 0.14285714285714285, 'fmeasure': 0.1255813953488372}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "THE BAY AREA QUAKE  In the aftermath of the California  earthquake  President Bush and his aides flew into a whirlwind of earthquake related activity yesterday morning.Some of it was necessary to get  federal help flowing to victims  but some seemed designed mostly to project an  image of a White House in action.With Congress already moving to grant  up to 3 billion in relief  earthquake victims need to know that state officials  are prepared to come up with the states share of disaster and reconstruction  aid as soon as possible  and as soon as is practical.Roads and bridges in the Bay area  appear to have suffered some of the most costly damage.In the Bay Area  that includes  hundreds of thousands of commuters who did not suffer personal damage  but face  massive obstacles in getting to work.He is estimating this weeks disaster  will generate insured losses of 2 billion to 4 billion  following about 4  billion in costs to insurers from Hurricane Hugo.The organization estimated residential  damages from Tuesdays earthquake at 500 million in the Bay area  with between  100 million to 150 million insured\n",
      "PREDICTED EXTRACT : \n",
      "The Association of California Insurance Cos . in Sacramento said that in the San Francisco area roughly 25 to 30 percent of the homes have earthquake insurance . Insurers arent required to offer earthquake insurance to commercial property owners  but the percentage of business property with the coverage is very high  industry spokesmen said . Bush visiting the California earthquake site this weekend . Insurers face the prospect of paying out billions of dollars for damages caused by this weeks California earthquake . the largest home and auto insurer in California  believes the losses from the earthquake could be somewhat less than the 475 million in damages it expects to pay out for claims resulting from Hurricane Hugo . Earthquake insurance is sold as a separate policy or a specific endorsement rider on a homeowners policy in California  because of the areas vulnerability to earthquakes . State Farm said about 25 of its policyholders in California have also purchased earthquake insurance . The Association of California Insurance Companies estimated damage to residential property could total 500 million  but only 100 million to 150 million is insured  it said . Industry officials say the Bay Bridge  unlike some bridges  has no earthquake coverage  either  so the cost of repairing it probably would have to be paid out of state general operating funds . Reinsurance is protection taken out by the insurance firms themselves . British  West German  Scandinavian and other overseas insurers are bracing for big claims from the San Francisco earthquake disaster . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.36099585062240663, 'recall': 0.4603174603174603, 'fmeasure': 0.4046511627906977}, 'rouge2': {'precision': 0.06666666666666667, 'recall': 0.0851063829787234, 'fmeasure': 0.07476635514018691}, 'rougeL': {'precision': 0.15767634854771784, 'recall': 0.20105820105820105, 'fmeasure': 0.17674418604651163}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "THE BAY AREA QUAKE  In the aftermath of the California  earthquake  President Bush and his aides flew into a whirlwind of earthquake related activity yesterday morning.Some of it was necessary to get  federal help flowing to victims  but some seemed designed mostly to project an  image of a White House in action.With Congress already moving to grant  up to 3 billion in relief  earthquake victims need to know that state officials  are prepared to come up with the states share of disaster and reconstruction  aid as soon as possible  and as soon as is practical.Roads and bridges in the Bay area  appear to have suffered some of the most costly damage.In the Bay Area  that includes  hundreds of thousands of commuters who did not suffer personal damage  but face  massive obstacles in getting to work.He is estimating this weeks disaster  will generate insured losses of 2 billion to 4 billion  following about 4  billion in costs to insurers from Hurricane Hugo.The organization estimated residential  damages from Tuesdays earthquake at 500 million in the Bay area  with between  100 million to 150 million insured\n",
      "PREDICTED EXTRACT : \n",
      "Insured homeowners without earthquake protection will get reimbursed only if their homes were ravaged by fire  which is covered under basic homeowner insurance polices  said Hugh Strawn  director of catastrophe services at the Property Loss Research Bureau in Schaumburg  Ill . Industry analysts predicted insurers would be able to reverse three years of declining rates and win rate hikes from state regulators due to the quake damages and the estimated 4 billion in damages from Hurricane Hugo  which hammered South Carolina and other parts of the southeastern United States earlier this month . Roughly twothirds of all insurance policies written in the United States are sold in California  where many of the nations most damaging earthquakes have occurred . While Deukmejian is not necessarily blaming Caltrans  he seems to  be preparing himself for such investigatory questions as  What did you know  and when did you know it  P P The governors answer  which he has been repeating ever since the day of the  quake  was that no one in Caltrans ever told him a freeway might collapse in an  earthquake . But no  one who has lived in California more than a few months would rule out the  chance that any bridge or freeway could collapse in the right kind of  earthquake . But investors are betting that the financial and psychological impact of the earthquake  coming so soon after the hurricane  will help stem more than two years of intense pricecutting wars among business insurers . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.35269709543568467, 'recall': 0.4497354497354497, 'fmeasure': 0.39534883720930236}, 'rouge2': {'precision': 0.041666666666666664, 'recall': 0.05319148936170213, 'fmeasure': 0.04672897196261682}, 'rougeL': {'precision': 0.11203319502074689, 'recall': 0.14285714285714285, 'fmeasure': 0.1255813953488372}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "THE BAY AREA QUAKE  In the aftermath of the California  earthquake  President Bush and his aides flew into a whirlwind of earthquake related activity yesterday morning.Some of it was necessary to get  federal help flowing to victims  but some seemed designed mostly to project an  image of a White House in action.With Congress already moving to grant  up to 3 billion in relief  earthquake victims need to know that state officials  are prepared to come up with the states share of disaster and reconstruction  aid as soon as possible  and as soon as is practical.Roads and bridges in the Bay area  appear to have suffered some of the most costly damage.In the Bay Area  that includes  hundreds of thousands of commuters who did not suffer personal damage  but face  massive obstacles in getting to work.He is estimating this weeks disaster  will generate insured losses of 2 billion to 4 billion  following about 4  billion in costs to insurers from Hurricane Hugo.The organization estimated residential  damages from Tuesdays earthquake at 500 million in the Bay area  with between  100 million to 150 million insured\n",
      "PREDICTED EXTRACT : \n",
      "Insured homeowners without earthquake protection will get reimbursed only if their homes were ravaged by fire  which is covered under basic homeowner insurance polices  said Hugh Strawn  director of catastrophe services at the Property Loss Research Bureau in Schaumburg  Ill . We dont think any company is going to have problems paying claims  said Elisa Siegal  public affairs manager for the American Insurance Association  a Washingtonbased trade group . Industry analysts predicted insurers would be able to reverse three years of declining rates and win rate hikes from state regulators due to the quake damages and the estimated 4 billion in damages from Hurricane Hugo  which hammered South Carolina and other parts of the southeastern United States earlier this month . P P P P At first I thought the smoke was fog . Bush visiting the California earthquake site this weekend . Insurers face the prospect of paying out billions of dollars for damages caused by this weeks California earthquake . State Farm said about 25 of its policyholders in California have also purchased earthquake insurance . The Association of California Insurance Companies estimated damage to residential property could total 500 million  but only 100 million to 150 million is insured  it said . Such increased demand for reinsurance  along with the losses the reinsurers will bear from these two disasters  are likely to spur increases in reinsurance prices that will later be translated into an overall price rise . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.34913793103448276, 'recall': 0.42857142857142855, 'fmeasure': 0.3847980997624703}, 'rouge2': {'precision': 0.06926406926406926, 'recall': 0.0851063829787234, 'fmeasure': 0.07637231503579953}, 'rougeL': {'precision': 0.14224137931034483, 'recall': 0.1746031746031746, 'fmeasure': 0.15676959619952496}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "THE BAY AREA QUAKE  In the aftermath of the California  earthquake  President Bush and his aides flew into a whirlwind of earthquake related activity yesterday morning.Some of it was necessary to get  federal help flowing to victims  but some seemed designed mostly to project an  image of a White House in action.With Congress already moving to grant  up to 3 billion in relief  earthquake victims need to know that state officials  are prepared to come up with the states share of disaster and reconstruction  aid as soon as possible  and as soon as is practical.Roads and bridges in the Bay area  appear to have suffered some of the most costly damage.In the Bay Area  that includes  hundreds of thousands of commuters who did not suffer personal damage  but face  massive obstacles in getting to work.He is estimating this weeks disaster  will generate insured losses of 2 billion to 4 billion  following about 4  billion in costs to insurers from Hurricane Hugo.The organization estimated residential  damages from Tuesdays earthquake at 500 million in the Bay area  with between  100 million to 150 million insured\n",
      "PREDICTED EXTRACT : \n",
      "The Association of California Insurance Cos . in Sacramento said that in the San Francisco area roughly 25 to 30 percent of the homes have earthquake insurance . Insurers arent required to offer earthquake insurance to commercial property owners  but the percentage of business property with the coverage is very high  industry spokesmen said . Bush visiting the California earthquake site this weekend . Insurers face the prospect of paying out billions of dollars for damages caused by this weeks California earthquake . the largest home and auto insurer in California  believes the losses from the earthquake could be somewhat less than the 475 million in damages it expects to pay out for claims resulting from Hurricane Hugo . Earthquake insurance is sold as a separate policy or a specific endorsement rider on a homeowners policy in California  because of the areas vulnerability to earthquakes . State Farm said about 25 of its policyholders in California have also purchased earthquake insurance . The Association of California Insurance Companies estimated damage to residential property could total 500 million  but only 100 million to 150 million is insured  it said . Industry officials say the Bay Bridge  unlike some bridges  has no earthquake coverage  either  so the cost of repairing it probably would have to be paid out of state general operating funds . Reinsurance is protection taken out by the insurance firms themselves . British  West German  Scandinavian and other overseas insurers are bracing for big claims from the San Francisco earthquake disaster . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.36099585062240663, 'recall': 0.4603174603174603, 'fmeasure': 0.4046511627906977}, 'rouge2': {'precision': 0.06666666666666667, 'recall': 0.0851063829787234, 'fmeasure': 0.07476635514018691}, 'rougeL': {'precision': 0.15767634854771784, 'recall': 0.20105820105820105, 'fmeasure': 0.17674418604651163}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "THE BAY AREA QUAKE  In the aftermath of the California  earthquake  President Bush and his aides flew into a whirlwind of earthquake related activity yesterday morning.Some of it was necessary to get  federal help flowing to victims  but some seemed designed mostly to project an  image of a White House in action.With Congress already moving to grant  up to 3 billion in relief  earthquake victims need to know that state officials  are prepared to come up with the states share of disaster and reconstruction  aid as soon as possible  and as soon as is practical.Roads and bridges in the Bay area  appear to have suffered some of the most costly damage.In the Bay Area  that includes  hundreds of thousands of commuters who did not suffer personal damage  but face  massive obstacles in getting to work.He is estimating this weeks disaster  will generate insured losses of 2 billion to 4 billion  following about 4  billion in costs to insurers from Hurricane Hugo.The organization estimated residential  damages from Tuesdays earthquake at 500 million in the Bay area  with between  100 million to 150 million insured\n",
      "PREDICTED EXTRACT : \n",
      "Most San Franciscoarea homeowners may have to pay for damage from Tuesdays earthquake out of their own pockets  while insurance companies may reap longterm benefits from higher rates  industry spokesmen and analysts said Wednesday . The Association of California Insurance Cos . P P P P This upper deck was absolutely shaking  the light stanchions were blowing back  and forth . With Congress already moving to grant up to 3 billion  in relief  earthquake victims need to know that state officials are prepared to  come up with the states share of disaster and reconstruction aid as soon as  possible  and as soon as is practical . While Deukmejian is not necessarily blaming Caltrans  he seems to  be preparing himself for such investigatory questions as  What did you know  and when did you know it  P P The governors answer  which he has been repeating ever since the day of the  quake  was that no one in Caltrans ever told him a freeway might collapse in an  earthquake . The earthquakes toll  including possible deep structural damage  goes far beyond the more easily observed damage from a hurricane  says George Reider  a vice president in Aetna Life amp  Casualty Insurance Co . State Farm said about 25 of its policyholders in California have also purchased earthquake insurance . He is estimating this weeks disaster will generate insured losses of 2 billion to 4 billion  following about 4 billion in costs to insurers from Hurricane Hugo . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.4810126582278481, 'recall': 0.6031746031746031, 'fmeasure': 0.5352112676056338}, 'rouge2': {'precision': 0.3093220338983051, 'recall': 0.3882978723404255, 'fmeasure': 0.3443396226415094}, 'rougeL': {'precision': 0.35443037974683544, 'recall': 0.4444444444444444, 'fmeasure': 0.39436619718309857}}\n",
      "\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "THE BAY AREA QUAKE  In the aftermath of the California  earthquake  President Bush and his aides flew into a whirlwind of earthquake related activity yesterday morning.Some of it was necessary to get  federal help flowing to victims  but some seemed designed mostly to project an  image of a White House in action.With Congress already moving to grant  up to 3 billion in relief  earthquake victims need to know that state officials  are prepared to come up with the states share of disaster and reconstruction  aid as soon as possible  and as soon as is practical.Roads and bridges in the Bay area  appear to have suffered some of the most costly damage.In the Bay Area  that includes  hundreds of thousands of commuters who did not suffer personal damage  but face  massive obstacles in getting to work.He is estimating this weeks disaster  will generate insured losses of 2 billion to 4 billion  following about 4  billion in costs to insurers from Hurricane Hugo.The organization estimated residential  damages from Tuesdays earthquake at 500 million in the Bay area  with between  100 million to 150 million insured\n",
      "PREDICTED EXTRACT : \n",
      "Insured homeowners without earthquake protection will get reimbursed only if their homes were ravaged by fire  which is covered under basic homeowner insurance polices  said Hugh Strawn  director of catastrophe services at the Property Loss Research Bureau in Schaumburg  Ill . We dont think any company is going to have problems paying claims  said Elisa Siegal  public affairs manager for the American Insurance Association  a Washingtonbased trade group . Industry analysts predicted insurers would be able to reverse three years of declining rates and win rate hikes from state regulators due to the quake damages and the estimated 4 billion in damages from Hurricane Hugo  which hammered South Carolina and other parts of the southeastern United States earlier this month . P P P P At first I thought the smoke was fog . Bush visiting the California earthquake site this weekend . Insurers face the prospect of paying out billions of dollars for damages caused by this weeks California earthquake . State Farm said about 25 of its policyholders in California have also purchased earthquake insurance . The Association of California Insurance Companies estimated damage to residential property could total 500 million  but only 100 million to 150 million is insured  it said . Such increased demand for reinsurance  along with the losses the reinsurers will bear from these two disasters  are likely to spur increases in reinsurance prices that will later be translated into an overall price rise . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.34913793103448276, 'recall': 0.42857142857142855, 'fmeasure': 0.3847980997624703}, 'rouge2': {'precision': 0.06926406926406926, 'recall': 0.0851063829787234, 'fmeasure': 0.07637231503579953}, 'rougeL': {'precision': 0.14224137931034483, 'recall': 0.1746031746031746, 'fmeasure': 0.15676959619952496}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "THE BAY AREA QUAKE  In the aftermath of the California  earthquake  President Bush and his aides flew into a whirlwind of earthquake related activity yesterday morning.Some of it was necessary to get  federal help flowing to victims  but some seemed designed mostly to project an  image of a White House in action.With Congress already moving to grant  up to 3 billion in relief  earthquake victims need to know that state officials  are prepared to come up with the states share of disaster and reconstruction  aid as soon as possible  and as soon as is practical.Roads and bridges in the Bay area  appear to have suffered some of the most costly damage.In the Bay Area  that includes  hundreds of thousands of commuters who did not suffer personal damage  but face  massive obstacles in getting to work.He is estimating this weeks disaster  will generate insured losses of 2 billion to 4 billion  following about 4  billion in costs to insurers from Hurricane Hugo.The organization estimated residential  damages from Tuesdays earthquake at 500 million in the Bay area  with between  100 million to 150 million insured\n",
      "PREDICTED EXTRACT : \n",
      "The Association of California Insurance Cos . in Sacramento said that in the San Francisco area roughly 25 to 30 percent of the homes have earthquake insurance . Industry analysts predicted insurers would be able to reverse three years of declining rates and win rate hikes from state regulators due to the quake damages and the estimated 4 billion in damages from Hurricane Hugo  which hammered South Carolina and other parts of the southeastern United States earlier this month . P P P P It was like somebody slugging a punching bag . But no  one who has lived in California more than a few months would rule out the  chance that any bridge or freeway could collapse in the right kind of  earthquake . In the Bay Area  that includes hundreds of thousands of commuters  who did not suffer personal damage  but face massive obstacles in getting to  work . The earthquakes toll  including possible deep structural damage  goes far beyond the more easily observed damage from a hurricane  says George Reider  a vice president in Aetna Life amp  Casualty Insurance Co . the largest home and auto insurer in California  believes the losses from the earthquake could be somewhat less than the 475 million in damages it expects to pay out for claims resulting from Hurricane Hugo . Orin Kramer  an insurance consultant in New York  estimates that the 1906 San Francisco destruction  on an inflationadjusted basis  included insured losses of 5 . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.3771186440677966, 'recall': 0.4708994708994709, 'fmeasure': 0.41882352941176476}, 'rouge2': {'precision': 0.16170212765957448, 'recall': 0.20212765957446807, 'fmeasure': 0.1796690307328605}, 'rougeL': {'precision': 0.22033898305084745, 'recall': 0.2751322751322751, 'fmeasure': 0.2447058823529412}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "THE BAY AREA QUAKE  In the aftermath of the California  earthquake  President Bush and his aides flew into a whirlwind of earthquake related activity yesterday morning.Some of it was necessary to get  federal help flowing to victims  but some seemed designed mostly to project an  image of a White House in action.With Congress already moving to grant  up to 3 billion in relief  earthquake victims need to know that state officials  are prepared to come up with the states share of disaster and reconstruction  aid as soon as possible  and as soon as is practical.Roads and bridges in the Bay area  appear to have suffered some of the most costly damage.In the Bay Area  that includes  hundreds of thousands of commuters who did not suffer personal damage  but face  massive obstacles in getting to work.He is estimating this weeks disaster  will generate insured losses of 2 billion to 4 billion  following about 4  billion in costs to insurers from Hurricane Hugo.The organization estimated residential  damages from Tuesdays earthquake at 500 million in the Bay area  with between  100 million to 150 million insured\n",
      "PREDICTED EXTRACT : \n",
      "The Association of California Insurance Cos . in Sacramento said that in the San Francisco area roughly 25 to 30 percent of the homes have earthquake insurance . Industry analysts predicted insurers would be able to reverse three years of declining rates and win rate hikes from state regulators due to the quake damages and the estimated 4 billion in damages from Hurricane Hugo  which hammered South Carolina and other parts of the southeastern United States earlier this month . P P P P It was like somebody slugging a punching bag . But no  one who has lived in California more than a few months would rule out the  chance that any bridge or freeway could collapse in the right kind of  earthquake . In the Bay Area  that includes hundreds of thousands of commuters  who did not suffer personal damage  but face massive obstacles in getting to  work . The earthquakes toll  including possible deep structural damage  goes far beyond the more easily observed damage from a hurricane  says George Reider  a vice president in Aetna Life amp  Casualty Insurance Co . the largest home and auto insurer in California  believes the losses from the earthquake could be somewhat less than the 475 million in damages it expects to pay out for claims resulting from Hurricane Hugo . Orin Kramer  an insurance consultant in New York  estimates that the 1906 San Francisco destruction  on an inflationadjusted basis  included insured losses of 5 . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.3771186440677966, 'recall': 0.4708994708994709, 'fmeasure': 0.41882352941176476}, 'rouge2': {'precision': 0.16170212765957448, 'recall': 0.20212765957446807, 'fmeasure': 0.1796690307328605}, 'rougeL': {'precision': 0.22033898305084745, 'recall': 0.2751322751322751, 'fmeasure': 0.2447058823529412}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "THE BAY AREA QUAKE  In the aftermath of the California  earthquake  President Bush and his aides flew into a whirlwind of earthquake related activity yesterday morning.Some of it was necessary to get  federal help flowing to victims  but some seemed designed mostly to project an  image of a White House in action.With Congress already moving to grant  up to 3 billion in relief  earthquake victims need to know that state officials  are prepared to come up with the states share of disaster and reconstruction  aid as soon as possible  and as soon as is practical.Roads and bridges in the Bay area  appear to have suffered some of the most costly damage.In the Bay Area  that includes  hundreds of thousands of commuters who did not suffer personal damage  but face  massive obstacles in getting to work.He is estimating this weeks disaster  will generate insured losses of 2 billion to 4 billion  following about 4  billion in costs to insurers from Hurricane Hugo.The organization estimated residential  damages from Tuesdays earthquake at 500 million in the Bay area  with between  100 million to 150 million insured\n",
      "PREDICTED EXTRACT : \n",
      "The Association of California Insurance Cos . in Sacramento said that in the San Francisco area roughly 25 to 30 percent of the homes have earthquake insurance . Industry analysts predicted insurers would be able to reverse three years of declining rates and win rate hikes from state regulators due to the quake damages and the estimated 4 billion in damages from Hurricane Hugo  which hammered South Carolina and other parts of the southeastern United States earlier this month . P P P P It was like somebody slugging a punching bag . But no  one who has lived in California more than a few months would rule out the  chance that any bridge or freeway could collapse in the right kind of  earthquake . In the Bay Area  that includes hundreds of thousands of commuters  who did not suffer personal damage  but face massive obstacles in getting to  work . The earthquakes toll  including possible deep structural damage  goes far beyond the more easily observed damage from a hurricane  says George Reider  a vice president in Aetna Life amp  Casualty Insurance Co . the largest home and auto insurer in California  believes the losses from the earthquake could be somewhat less than the 475 million in damages it expects to pay out for claims resulting from Hurricane Hugo . Orin Kramer  an insurance consultant in New York  estimates that the 1906 San Francisco destruction  on an inflationadjusted basis  included insured losses of 5 . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.3771186440677966, 'recall': 0.4708994708994709, 'fmeasure': 0.41882352941176476}, 'rouge2': {'precision': 0.16170212765957448, 'recall': 0.20212765957446807, 'fmeasure': 0.1796690307328605}, 'rougeL': {'precision': 0.22033898305084745, 'recall': 0.2751322751322751, 'fmeasure': 0.2447058823529412}}\n",
      "\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "THE BAY AREA QUAKE  In the aftermath of the California  earthquake  President Bush and his aides flew into a whirlwind of earthquake related activity yesterday morning.Some of it was necessary to get  federal help flowing to victims  but some seemed designed mostly to project an  image of a White House in action.With Congress already moving to grant  up to 3 billion in relief  earthquake victims need to know that state officials  are prepared to come up with the states share of disaster and reconstruction  aid as soon as possible  and as soon as is practical.Roads and bridges in the Bay area  appear to have suffered some of the most costly damage.In the Bay Area  that includes  hundreds of thousands of commuters who did not suffer personal damage  but face  massive obstacles in getting to work.He is estimating this weeks disaster  will generate insured losses of 2 billion to 4 billion  following about 4  billion in costs to insurers from Hurricane Hugo.The organization estimated residential  damages from Tuesdays earthquake at 500 million in the Bay area  with between  100 million to 150 million insured\n",
      "PREDICTED EXTRACT : \n",
      "The Association of California Insurance Cos . in Sacramento said that in the San Francisco area roughly 25 to 30 percent of the homes have earthquake insurance . Industry analysts predicted insurers would be able to reverse three years of declining rates and win rate hikes from state regulators due to the quake damages and the estimated 4 billion in damages from Hurricane Hugo  which hammered South Carolina and other parts of the southeastern United States earlier this month . P P P P It was like somebody slugging a punching bag . But no  one who has lived in California more than a few months would rule out the  chance that any bridge or freeway could collapse in the right kind of  earthquake . In the Bay Area  that includes hundreds of thousands of commuters  who did not suffer personal damage  but face massive obstacles in getting to  work . The earthquakes toll  including possible deep structural damage  goes far beyond the more easily observed damage from a hurricane  says George Reider  a vice president in Aetna Life amp  Casualty Insurance Co . the largest home and auto insurer in California  believes the losses from the earthquake could be somewhat less than the 475 million in damages it expects to pay out for claims resulting from Hurricane Hugo . Orin Kramer  an insurance consultant in New York  estimates that the 1906 San Francisco destruction  on an inflationadjusted basis  included insured losses of 5 . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.3771186440677966, 'recall': 0.4708994708994709, 'fmeasure': 0.41882352941176476}, 'rouge2': {'precision': 0.16170212765957448, 'recall': 0.20212765957446807, 'fmeasure': 0.1796690307328605}, 'rougeL': {'precision': 0.22033898305084745, 'recall': 0.2751322751322751, 'fmeasure': 0.2447058823529412}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "REFERENCE EXTRACT : \n",
      "THE BAY AREA QUAKE  In the aftermath of the California  earthquake  President Bush and his aides flew into a whirlwind of earthquake related activity yesterday morning.Some of it was necessary to get  federal help flowing to victims  but some seemed designed mostly to project an  image of a White House in action.With Congress already moving to grant  up to 3 billion in relief  earthquake victims need to know that state officials  are prepared to come up with the states share of disaster and reconstruction  aid as soon as possible  and as soon as is practical.Roads and bridges in the Bay area  appear to have suffered some of the most costly damage.In the Bay Area  that includes  hundreds of thousands of commuters who did not suffer personal damage  but face  massive obstacles in getting to work.He is estimating this weeks disaster  will generate insured losses of 2 billion to 4 billion  following about 4  billion in costs to insurers from Hurricane Hugo.The organization estimated residential  damages from Tuesdays earthquake at 500 million in the Bay area  with between  100 million to 150 million insured\n",
      "PREDICTED EXTRACT : \n",
      "Most San Franciscoarea homeowners may have to pay for damage from Tuesdays earthquake out of their own pockets  while insurance companies may reap longterm benefits from higher rates  industry spokesmen and analysts said Wednesday . The Association of California Insurance Cos . P P P P This upper deck was absolutely shaking  the light stanchions were blowing back  and forth . With Congress already moving to grant up to 3 billion  in relief  earthquake victims need to know that state officials are prepared to  come up with the states share of disaster and reconstruction aid as soon as  possible  and as soon as is practical . While Deukmejian is not necessarily blaming Caltrans  he seems to  be preparing himself for such investigatory questions as  What did you know  and when did you know it  P P The governors answer  which he has been repeating ever since the day of the  quake  was that no one in Caltrans ever told him a freeway might collapse in an  earthquake . The earthquakes toll  including possible deep structural damage  goes far beyond the more easily observed damage from a hurricane  says George Reider  a vice president in Aetna Life amp  Casualty Insurance Co . State Farm said about 25 of its policyholders in California have also purchased earthquake insurance . He is estimating this weeks disaster will generate insured losses of 2 billion to 4 billion  following about 4 billion in costs to insurers from Hurricane Hugo . \n",
      "ROUGE SCORE       : \n",
      "{'rouge1': {'precision': 0.4810126582278481, 'recall': 0.6031746031746031, 'fmeasure': 0.5352112676056338}, 'rouge2': {'precision': 0.3093220338983051, 'recall': 0.3882978723404255, 'fmeasure': 0.3443396226415094}, 'rougeL': {'precision': 0.35443037974683544, 'recall': 0.4444444444444444, 'fmeasure': 0.39436619718309857}}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "\n",
      "**************************************************\n",
      "Checking if doc :  d063j passes all preprocessing steps ::....\n",
      "Preprocessing Step Cleared Successfully\n",
      "No of sentences =  247\n",
      "Document :  d063j Processing !!!--------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVersion In Progress :::\u001b[39m\u001b[38;5;124m\"\u001b[39m,version)\n\u001b[0;32m     65\u001b[0m params\u001b[38;5;241m=\u001b[39mparams_class(doc_threshold_value,sentence_threshold_value,spacy_use,restricted_docs_list,epsilon,L,no_of_objectives,pop_size,pm,trial_cutoff_limit,cycles,delta,num_topics,n_top_words,mutation_operator_version,replacement_operator_version,algorithm,IP_population_initialization,topic_enabled,sbert_enabled,num_keywords,Lextract,run,delim,kaggle,csv_name) \n\u001b[1;32m---> 66\u001b[0m d \u001b[38;5;241m=\u001b[39m process_all_files(params,file_paths_class)\n",
      "Cell \u001b[1;32mIn[39], line 91\u001b[0m, in \u001b[0;36mprocess_all_files\u001b[1;34m(params, file_paths_class)\u001b[0m\n\u001b[0;32m     89\u001b[0m compressed_doc_info\u001b[38;5;241m=\u001b[39mcompressed_doc_info_class(preprocessed_inputs,list_of_sentences,params\u001b[38;5;241m.\u001b[39mL,params\u001b[38;5;241m.\u001b[39mepsilon)\n\u001b[0;32m     90\u001b[0m graph_scores\u001b[38;5;241m=\u001b[39mGETS(compressed_doc_info)\u001b[38;5;241m.\u001b[39mgets_sentence_scores\n\u001b[1;32m---> 91\u001b[0m lxr\u001b[38;5;241m=\u001b[39mlexrank_model(dataset_access_path)\n\u001b[0;32m     92\u001b[0m lexrankscores_cont \u001b[38;5;241m=\u001b[39m lxr\u001b[38;5;241m.\u001b[39mrank_sentences(list_of_sentences,threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,fast_power_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     93\u001b[0m lexrankscores_dict\u001b[38;5;241m=\u001b[39m{}\n",
      "Cell \u001b[1;32mIn[32], line 71\u001b[0m, in \u001b[0;36mlexrank_model\u001b[1;34m(dataset_access_path)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39mopen(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m     69\u001b[0m         documents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mreadlines())\n\u001b[1;32m---> 71\u001b[0m lxr \u001b[38;5;241m=\u001b[39m LexRank(documents, stopwords\u001b[38;5;241m=\u001b[39mSTOPWORDS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lxr\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lexrank\\algorithms\\summarizer.py:30\u001b[0m, in \u001b[0;36mLexRank.__init__\u001b[1;34m(self, documents, stopwords, keep_numbers, keep_emails, keep_urls, include_new_words)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_urls \u001b[38;5;241m=\u001b[39m keep_urls\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_new_words \u001b[38;5;241m=\u001b[39m include_new_words\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_idf(documents)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lexrank\\algorithms\\summarizer.py:117\u001b[0m, in \u001b[0;36mLexRank._calculate_idf\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m    114\u001b[0m doc_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[1;32m--> 117\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_sentence(sentence)\n\u001b[0;32m    118\u001b[0m     doc_words\u001b[38;5;241m.\u001b[39mupdate(words)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc_words:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lexrank\\algorithms\\summarizer.py:100\u001b[0m, in \u001b[0;36mLexRank.tokenize_sentence\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_sentence\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence):\n\u001b[1;32m--> 100\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenize(\n\u001b[0;32m    101\u001b[0m         sentence,\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords,\n\u001b[0;32m    103\u001b[0m         keep_numbers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_numbers,\n\u001b[0;32m    104\u001b[0m         keep_emails\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_emails,\n\u001b[0;32m    105\u001b[0m         keep_urls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_urls,\n\u001b[0;32m    106\u001b[0m     )\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lexrank\\utils\\text.py:64\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(text, stopwords, keep_numbers, keep_emails, keep_urls)\u001b[0m\n\u001b[0;32m     61\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit():\n\u001b[1;32m---> 64\u001b[0m     emails \u001b[38;5;241m=\u001b[39m EMAIL_REGEX\u001b[38;5;241m.\u001b[39mfindall(word)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m emails:\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m keep_emails:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def dirs_execute():\n",
    "    try:\n",
    "        os.makedirs(\"/kaggle/working/results\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        os.makedirs(\"/kaggle/working/details\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "kaggle=False\n",
    "if kaggle==True:\n",
    "    dirs_execute()\n",
    "    base_path_input=\"/kaggle/input/duc2002dataandwheels/KaggleWheels\"\n",
    "    base_path_output=\"/kaggle/working\"\n",
    "    delim=\"/\"\n",
    "else:\n",
    "    base_path_input=r\"C:\\Users\\shari\\OneDrive\\Desktop\\Text Sum\"\n",
    "    base_path_output=r\"C:\\Users\\shari\\OneDrive\\Desktop\\Text Sum\"\n",
    "    delim=\"\\\\\"\n",
    "\n",
    "dirs_execute()\n",
    "current_datetime_string=return_current_datetime_string()\n",
    "    \n",
    "replacement_operators=[\"dominate\",\"rcd\",\"epsilon_dominate\"]\n",
    "mutation_operators=[1,2]\n",
    "algos=[\"NSGA\",\"MOABC\"]\n",
    "topic_enabled_versions=[\"v1\",\"v2\",\"v3\",\"v4\"]\n",
    "\n",
    "IP_population_initialization=False\n",
    "sbert_enabled=False\n",
    "csv_name=\"ReplacementOperatorExperiment_epsilondominate_0_2\"\n",
    "num_keywords=30\n",
    "\n",
    "inde_runs=10\n",
    "\n",
    "doc_threshold_value=-1\n",
    "sentence_threshold_value=500\n",
    "spacy_use=False\n",
    "cycles=2\n",
    "delta=0.1\n",
    "num_topics=5\n",
    "n_top_words=20\n",
    "restricted_docs_list=[]\n",
    "epsilon=50\n",
    "L=200\n",
    "Lextract=200\n",
    "no_of_objectives=2\n",
    "pop_size=50\n",
    "pm=0.4\n",
    "trial_cutoff_limit=5\n",
    "\n",
    "mutation_operator_version=mutation_operators[0]\n",
    "replacement_operator_version=replacement_operators[2]\n",
    "algorithm=algos[1]\n",
    "topic_enabled=topic_enabled_versions[2]\n",
    "\n",
    "\n",
    "for run in range(0,2,1):\n",
    "    version=algorithm+\"_\"+replacement_operator_version+\"_\"+\"MutationOperatorVersion=\"+str(mutation_operator_version)+\"_\"+\"IndependentRun=\"+str(run)+\"_\"+\"topic-enabled=\"+str(topic_enabled)+\"_\"+\"IP-population-initialization=\"+str(IP_population_initialization)+\"_\"+\"sbert-enabled=\"+str(sbert_enabled)\n",
    "    file_paths_class=file_paths(base_path_input,base_path_output,version,current_datetime_string,kaggle)\n",
    "    print(\"Version In Progress :::\",version)\n",
    "    params=params_class(doc_threshold_value,sentence_threshold_value,spacy_use,restricted_docs_list,epsilon,L,no_of_objectives,pop_size,pm,trial_cutoff_limit,cycles,delta,num_topics,n_top_words,mutation_operator_version,replacement_operator_version,algorithm,IP_population_initialization,topic_enabled,sbert_enabled,num_keywords,Lextract,run,delim,kaggle,csv_name) \n",
    "    d = process_all_files(params,file_paths_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T20:00:21.194548Z",
     "iopub.status.busy": "2024-03-17T20:00:21.193519Z",
     "iopub.status.idle": "2024-03-17T20:00:21.201478Z",
     "shell.execute_reply": "2024-03-17T20:00:21.200427Z",
     "shell.execute_reply.started": "2024-03-17T20:00:21.194504Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4612038,
     "isSourceIdPinned": false,
     "sourceId": 7861953,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
